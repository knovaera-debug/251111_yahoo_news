# -*- coding: utf-8 -*-
"""
çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå›½å†…8ç¤¾å¯¾å¿œç‰ˆï¼‰ - æœ€çµ‚è¨­å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼š
1. keywords.txtã‹ã‚‰å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚’èª­ã¿è¾¼ã¿ã€é †æ¬¡Yahooã‚·ãƒ¼ãƒˆã«è¨˜äº‹ãƒªã‚¹ãƒˆã‚’è¿½è¨˜ (A-Dåˆ—)ã€‚
2. æŠ•ç¨¿æ—¥æ™‚ã‹ã‚‰æ›œæ—¥ã‚’ç¢ºå®Ÿã«å‰Šé™¤ã—ã€ã‚¯ãƒªãƒ¼ãƒ³ãªå½¢å¼ã§æ ¼ç´ã€‚
3. æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—ã—ã€è¡Œã”ã¨ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å³æ™‚åæ˜ ã€‚
Â  Â  -> ã€æ”¹ä¿®ã€‘è¨˜äº‹æœ¬æ–‡ã¯1ãƒšãƒ¼ã‚¸1ã‚»ãƒ«ã§E-Nåˆ—ã«æ ¼ç´ã€‚
Â  Â  -> ã€æ”¹ä¿®ã€‘ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã¯Oåˆ—ã«æ ¼ç´ã€‚
Â  Â  -> ã€æ”¹ä¿®ã€‘ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã¯1ãƒšãƒ¼ã‚¸1ã‚»ãƒ«ã§S-ACåˆ—ã«æ ¼ç´ã€‚
4. å…¨è¨˜äº‹ã‚’æŠ•ç¨¿æ—¥ã®æ–°ã—ã„é †ã«ä¸¦ã³æ›¿ãˆ (Cåˆ—åŸºæº–)ã€‚
5. ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸè¨˜äº‹ã«å¯¾ã—ã€æ–°ã—ã„ã‚‚ã®ã‹ã‚‰Geminiåˆ†æ (P-Råˆ—, AD-AEåˆ—) ã‚’å®Ÿè¡Œã€‚
Â  Â  -> ã€æ”¹ä¿®ã€‘APIæ¶ˆè²»é‡å¯¾ç­–ã®ãŸã‚ã€1å›ã®å®Ÿè¡Œã§åˆ†æã™ã‚‹ä»¶æ•°ã‚’åˆ¶é™ã€‚
"""

import os
import json
import time
import re
import random
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Optional, Set, Dict, Any
import sys
from urllib.parse import urlparse, parse_qs, urlunparse, urlencode # è¿½åŠ 

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

# --- Gemini API é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from google import genai
from google.genai import types
from google.api_core.exceptions import ResourceExhausted
# ------------------------------------

# ====== è¨­å®š ======
# â–¼â–¼â–¼ ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«ã‚ˆã‚Šä¿®æ­£ â–¼â–¼â–¼
SHARED_SPREADSHEET_ID = "1FlQmR1Xe25wCLi-zt-lnigDoh8Qz8RUeJbWZifuDW84"
# â–²â–²â–² ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«ã‚ˆã‚Šä¿®æ­£ â–²â–²â–²
KEYWORD_FILE = "keywords.txt"
SOURCE_SPREADSHEET_ID = SHARED_SPREADSHEET_ID
SOURCE_SHEET_NAME = "Yahoo"
DEST_SPREADSHEET_ID = SHARED_SPREADSHEET_ID
# æ›œæ—¥å‰Šé™¤ã®å¯¾è±¡ã¨ã™ã‚‹æœ€å¤§è¡Œæ•°ã‚’10000ã«è¨­å®š
MAX_SHEET_ROWS_FOR_REPLACE = 10000
MAX_PAGES = 10 # è¨˜äº‹æœ¬æ–‡å–å¾—ã®æœ€å¤§å·¡å›ãƒšãƒ¼ã‚¸æ•° (â€»ãƒ­ã‚¸ãƒƒã‚¯æ”¹ä¿®ã«ã‚ˆã‚Šç¾åœ¨ã¯1ãƒšãƒ¼ã‚¸ã®ã¿å–å¾—)

# â–¼â–¼â–¼ã€å¤‰æ›´ã€‘ Eåˆ—ã‚’ã€Œæœ¬æ–‡P1ã€ï½ã€Œæœ¬æ–‡P10ã€ã®10åˆ—ã«å¤‰æ›´ã—ã€ä»¥é™ã®åˆ—ã‚’ã™ã¹ã¦ã‚·ãƒ•ãƒˆ (å…¨31åˆ—) â–¼â–¼â–¼
YAHOO_SHEET_HEADERS = [
    # åŸºæœ¬æƒ…å ± (A-D)
    "URL", "ã‚¿ã‚¤ãƒˆãƒ«", "æŠ•ç¨¿æ—¥æ™‚", "ã‚½ãƒ¼ã‚¹", 
    # è¨˜äº‹æœ¬æ–‡ (E-N)
    "æœ¬æ–‡P1", "æœ¬æ–‡P2", "æœ¬æ–‡P3", "æœ¬æ–‡P4", "æœ¬æ–‡P5", 
    "æœ¬æ–‡P6", "æœ¬æ–‡P7", "æœ¬æ–‡P8", "æœ¬æ–‡P9", "æœ¬æ–‡P10",
    # ã‚³ãƒ¡ãƒ³ãƒˆæ•° (O)
    "ã‚³ãƒ¡ãƒ³ãƒˆæ•°",
    # åŸºæœ¬AIåˆ†æ (P-R)
    "å¯¾è±¡ä¼æ¥­", "ã‚«ãƒ†ã‚´ãƒªåˆ†é¡", "ãƒã‚¸ãƒã‚¬åˆ†é¡",
    # ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ (S-AC)
    "ã‚³ãƒ¡ãƒ³ãƒˆP1", "ã‚³ãƒ¡ãƒ³ãƒˆP2", "ã‚³ãƒ¡ãƒ³ãƒˆP3", "ã‚³ãƒ¡ãƒ³ãƒˆP4", "ã‚³ãƒ¡ãƒ³ãƒˆP5", 
    "ã‚³ãƒ¡ãƒ³ãƒˆP6", "ã‚³ãƒ¡ãƒ³ãƒˆP7", "ã‚³ãƒ¡ãƒ³ãƒˆP8", "ã‚³ãƒ¡ãƒ³ãƒˆP9", "ã‚³ãƒ¡ãƒ³ãƒˆP10", 
    "ã‚³ãƒ¡ãƒ³ãƒˆP11(ä»¥é™)",
    # æ—¥ç”£é–¢é€£AIåˆ†æ (AD-AE)
    "æ—¥ç”£é–¢é€£è¨€åŠ", "æ—¥ç”£è¦–ç‚¹ãƒã‚¸ãƒã‚¬"
]
# â–²â–²â–²ã€å¤‰æ›´ã€‘ Eåˆ—ã‚’ã€Œæœ¬æ–‡P1ã€ï½ã€Œæœ¬æ–‡P10ã€ã®10åˆ—ã«å¤‰æ›´ã—ã€ä»¥é™ã®åˆ—ã‚’ã™ã¹ã¦ã‚·ãƒ•ãƒˆ (å…¨31åˆ—) â–²â–²â–²

REQ_HEADERS = {"User-Agent": "Mozilla/5.0"}
TZ_JST = timezone(timedelta(hours=9))

# â–¼â–¼â–¼ã€ä¿®æ­£ã€‘ æ—¥ç”£é–¢é€£ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ  â–¼â–¼â–¼
PROMPT_FILES = [
    "prompt_gemini_role.txt",
    "prompt_posinega.txt",
    "prompt_category.txt",
    "prompt_target_company.txt",
    "prompt_nissan_mention.txt",    # <-- è¿½åŠ 
    "prompt_nissan_sentiment.txt"   # <-- è¿½åŠ 
]
# â–²â–²â–²ã€ä¿®æ­£ã€‘ æ—¥ç”£é–¢é€£ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ  â–²â–²â–²

try:
    # â–¼â–¼â–¼ ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«ã‚ˆã‚Šä¿®æ­£ (APIã‚­ãƒ¼è¨­å®šã‚’è¿½åŠ ) â–¼â–¼â–¼
    # APIã‚­ãƒ¼ã‚’ã“ã“ã§è¨­å®š
    genai.configure(api_key="AIzaSyCwNV4NgFl1-9yxEXqr-QJs-F7X4QjmyNQ")
    
    GEMINI_CLIENT = genai.Client()
    # â–²â–²â–² ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«ã‚ˆã‚Šä¿®æ­£ â–²â–²â–²
except Exception as e:
    print(f"è­¦å‘Š: Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Geminiåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
    GEMINI_CLIENT = None

GEMINI_PROMPT_TEMPLATE = None

# ====== ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ ======

# ã€ä¿®æ­£ç‚¹ã€‘gspread.utils.col_to_letter ã®ä»£æ›¿é–¢æ•°ã‚’å®šç¾©
def gspread_util_col_to_letter(col_index: int) -> str:
    """ gspreadã®å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ col_to_letter ãŒãªã„å ´åˆã®ä»£æ›¿é–¢æ•° (1-indexed) """
    if col_index < 1:
        raise ValueError("Column index must be 1 or greater")
    
    # gspread.utils.rowcol_to_a1(1, col_index) ã‚’åˆ©ç”¨ã—ã¦A1è¡¨è¨˜ã‚’å–å¾—ã—ã€è¡Œç•ªå·ã‚’å‰Šé™¤ã—ã¦åˆ—æ–‡å­—ã®ã¿ã‚’æŠ½å‡º
    a1_notation = gspread.utils.rowcol_to_a1(1, col_index)
    return re.sub(r'\d+', '', a1_notation)

def jst_now() -> datetime:
    return datetime.now(TZ_JST)

def format_datetime(dt_obj) -> str:
    # ã€ä¿®æ­£ç‚¹â‘ ã€‘æ—¥æ™‚ã®è¡¨ç¤ºå½¢å¼ã‚’ yyyy/mm/dd hh:mm:ss ã«å¤‰æ›´
    return dt_obj.strftime("%Y/%m/%d %H:%M:%S") # 2025/10/08 10:00:28 ã®å½¢å¼

def parse_post_date(raw, today_jst: datetime) -> Optional[datetime]:
    if raw is None: return None
    if isinstance(raw, str):
        s = raw.strip()
        
        # æ›œæ—¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‰Šé™¤ã™ã‚‹æ­£è¦è¡¨ç¾ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œ
        s = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", s).strip()
        
        # é…ä¿¡ã¨ã„ã†æ–‡å­—ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã¯å‰Šé™¤
        s = s.replace('é…ä¿¡', '').strip()
        
        # ä¿®æ­£å¾Œã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å«ã‚ã¦ãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹
        for fmt in ("%Y/%m/%d %H:%M:%S", "%y/%m/%d %H:%M", "%m/%d %H:%M", "%Y/%m/%d %H:%M"):
            try:
                dt = datetime.strptime(s, fmt)
                if fmt == "%m/%d %H:%M":
                    # å¹´ãŒãªã„å½¢å¼ã®å ´åˆã€ä»Šå¹´ã‚’é©ç”¨
                    dt = dt.replace(year=today_jst.year)
                
                # å¹´ãŒæœªæ¥ï¼ˆç¾åœ¨æœˆã®ç¿Œæœˆä»¥é™ï¼‰ã§ã‚ã‚Œã°ã€å‰å¹´ã«ä¿®æ­£ã™ã‚‹ (æœˆæ—¥ã®ã¿ã®å½¢å¼ã‚’è€ƒæ…®)
                if dt.replace(tzinfo=TZ_JST) > today_jst + timedelta(days=31):
                    dt = dt.replace(year=dt.year - 1)
                    
                return dt.replace(tzinfo=TZ_JST)
            except ValueError:
                pass
        return None

def build_gspread_client() -> gspread.Client:
    try:
        # ç’°å¢ƒå¤‰æ•° GCP_SERVICE_ACCOUNT_KEY ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€
        creds_str = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        
        if creds_str:
            info = json.loads(creds_str)
            credentials = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
            return gspread.authorize(credentials)
        else:
            # GCP_SERVICE_ACCOUNT_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«èªè¨¼ã‚’è©¦ã¿ã‚‹ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
            try:
                return gspread.service_account(filename='credentials.json')
            except FileNotFoundError:
                raise RuntimeError("Googleèªè¨¼æƒ…å ± (GCP_SERVICE_ACCOUNT_KEY)ãŒç’°å¢ƒå¤‰æ•°ã€ã¾ãŸã¯ 'credentials.json' ãƒ•ã‚¡ã‚¤ãƒ«ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    except Exception as e:
        raise RuntimeError(f"Googleèªè¨¼ã«å¤±æ•—: {e}")

def load_keywords(filename: str) -> List[str]:
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)
        with open(file_path, 'r', encoding='utf-8') as f:
            keywords = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        if not keywords:
            raise ValueError("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã«æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return keywords
    except FileNotFoundError:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return []
    except Exception as e:
        print(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def load_gemini_prompt() -> str:
    global GEMINI_PROMPT_TEMPLATE
    if GEMINI_PROMPT_TEMPLATE is not None:
        return GEMINI_PROMPT_TEMPLATE
        
    combined_instructions = []
    
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        role_instruction = ""

        role_file = PROMPT_FILES[0]
        file_path = os.path.join(script_dir, role_file)
        with open(file_path, 'r', encoding='utf-8') as f:
            role_instruction = f.read().strip()
        
        for filename in PROMPT_FILES[1:]:
            file_path = os.path.join(script_dir, filename)
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            if content:
                combined_instructions.append(content)
                        
        if not role_instruction or not combined_instructions:
            print("è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒä¸å®Œå…¨ã¾ãŸã¯ç©ºã§ã™ã€‚")
            return ""

        base_prompt = role_instruction + "\n" + "\n".join(combined_instructions)
        base_prompt += "\n\nè¨˜äº‹æœ¬æ–‡:\n{TEXT_TO_ANALYZE}"

        GEMINI_PROMPT_TEMPLATE = base_prompt
        print(f" Geminiãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ {PROMPT_FILES} ã‹ã‚‰èª­ã¿è¾¼ã¿ã€çµåˆã—ã¾ã—ãŸã€‚")
        return base_prompt
        
    except FileNotFoundError as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€éƒ¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«å: {e.filename}")
        return ""
    except Exception as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return ""

def request_with_retry(url: str, max_retries: int = 3) -> Optional[requests.Response]:
    """ è¨˜äº‹æœ¬æ–‡å–å¾—ç”¨ã®ãƒªãƒˆãƒ©ã‚¤ä»˜ããƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒ«ãƒ‘ãƒ¼ """
    for attempt in range(max_retries):
        try:
            res = requests.get(url, headers=REQ_HEADERS, timeout=20)
            
            # ğŸ’¡ æ”¹ä¿®ç‚¹â‘¡: 404 Client Error ã®å ´åˆã€ãƒªãƒˆãƒ©ã‚¤ã›ãš None ã‚’è¿”ã—ã¦å³åº§ã«ã‚¹ã‚­ãƒƒãƒ—
            if res.status_code == 404:
                print(f"  âŒ ãƒšãƒ¼ã‚¸ãªã— (404 Client Error): {url}")
                return None
                
            res.raise_for_status()
            return res
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt + random.random()
                print(f"  âš ï¸ æ¥ç¶šã‚¨ãƒ©ãƒ¼ã€ãƒªãƒˆãƒ©ã‚¤ä¸­... ({attempt + 1}/{max_retries})ã€‚å¾…æ©Ÿ: {wait_time:.2f}ç§’")
                time.sleep(wait_time)
            else:
                print(f"  âŒ æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤å¤±æ•—: {e}")
                return None
    return None

# ====== Gemini åˆ†æé–¢æ•° (ã€ä¿®æ­£ã€‘æ—¥ç”£é–¢é€£ã®åˆ†æã‚­ãƒ¼ã‚’è¿½åŠ ) ======
def analyze_with_gemini(text_to_analyze: str) -> Tuple[str, str, str, str, str, bool]:
    """
    ã€ä¿®æ­£ã€‘
    - æˆ»ã‚Šå€¤ã« nissan_mention, nissan_sentiment ã‚’è¿½åŠ ã€‚
    - response_schema ã«æ–°ã—ã„ã‚­ãƒ¼ã‚’è¿½åŠ ã€‚
    """
    if not GEMINI_CLIENT:
        return "N/A", "N/A", "N/A", "N/A", "N/A", False
        
    if not text_to_analyze.strip():
        return "N/A", "N/A", "N/A", "N/A", "N/A", False

    prompt_template = load_gemini_prompt()
    if not prompt_template:
        return "ERROR(Prompt Missing)", "ERROR", "ERROR", "ERROR", "ERROR", False

    MAX_RETRIES = 3
    MAX_CHARACTERS = 15000
    
    for attempt in range(MAX_RETRIES):
        try:
            text_for_prompt = text_to_analyze[:MAX_CHARACTERS]
            prompt = prompt_template.replace("{TEXT_TO_ANALYZE}", text_for_prompt)
            
            response = GEMINI_CLIENT.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    # â–¼â–¼â–¼ã€ä¿®æ­£ã€‘ ã‚¹ã‚­ãƒ¼ãƒã«5ã¤ã®ã‚­ãƒ¼ã‚’å®šç¾© â–¼â–¼â–¼
                    response_schema={"type": "object", "properties": {
                        "company_info": {"type": "string", "description": "è¨˜äº‹ã®ä¸»é¡Œä¼æ¥­åã¨ï¼ˆï¼‰å†…ã«å…±åŒé–‹ç™ºä¼æ¥­åã‚’è¨˜è¼‰ã—ãŸçµæœ"},
                        "category": {"type": "string", "description": "ä¼æ¥­ã€ãƒ¢ãƒ‡ãƒ«ã€æŠ€è¡“ãªã©ã®åˆ†é¡çµæœ"},
                        "sentiment": {"type": "string", "description": "ãƒã‚¸ãƒ†ã‚£ãƒ–ã€ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã®ã„ãšã‚Œã‹"},
                        "nissan_mention": {"type": "string", "description": "æ—¥ç”£é–¢é€£ã®è¨€åŠï¼ˆã‚ã‚Œã°ï¼‰"},
                        "nissan_sentiment": {"type": "string", "description": "æ—¥ç”£è¦–ç‚¹ã§ã®ãƒã‚¸ãƒã‚¬åˆ¤å®šï¼ˆã‚ã‚Œã°ï¼‰"}
                    }}
                    # â–²â–²â–²ã€ä¿®æ­£ã€‘ ã‚¹ã‚­ãƒ¼ãƒã«5ã¤ã®ã‚­ãƒ¼ã‚’å®šç¾© â–²â–²â–²
                ),
            )

            analysis = json.loads(response.text.strip())
            
            # â–¼â–¼â–¼ã€ä¿®æ­£ã€‘ 5ã¤ã®ã‚­ãƒ¼ã‚’å–å¾— â–¼â–¼â–¼
            company_info = analysis.get("company_info", "N/A")
            category = analysis.get("category", "N/A")
            sentiment = analysis.get("sentiment", "N/A")
            nissan_mention = analysis.get("nissan_mention", "N/A")
            nissan_sentiment = analysis.get("nissan_sentiment", "N/A")

            return company_info, category, sentiment, nissan_mention, nissan_sentiment, False
            # â–²â–²â–²ã€ä¿®æ­£ã€‘ 5ã¤ã®ã‚­ãƒ¼ã‚’å–å¾— â–²â–²â–²

        # ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ã‚’æœ€å„ªå…ˆã§æ•æ‰ã—ã€å¼·åˆ¶çµ‚äº†
        except ResourceExhausted as e:
            print(f"  ğŸš¨ Gemini API ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ (429): {e}")
            print("\n===== ğŸ›‘ ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚’æ¤œå‡ºã—ãŸãŸã‚ã€ã‚·ã‚¹ãƒ†ãƒ ã‚’ç›´ã¡ã«ä¸­æ–­ã—ã¾ã™ã€‚ =====")
            sys.stdout.flush()
            sys.exit(1) # ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’çµ‚äº†

        # ã‚¯ã‚©ãƒ¼ã‚¿ä»¥å¤–ã®ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ã®ã¿ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã¨ã™ã‚‹
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                wait_time = 2 ** attempt + random.random()
                print(f"  âš ï¸ Gemini API ä¸€æ™‚çš„ãªæ¥ç¶šã¾ãŸã¯å‡¦ç†ã‚¨ãƒ©ãƒ¼ã€‚{wait_time:.2f} ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™ (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES})ã€‚")
                time.sleep(wait_time)
                continue
            else:
                print(f"Geminiåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                return "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", False
        
    return "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", False

# ====== ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•° (ã‚½ãƒ¼ã‚¹æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£) ======

def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword})...")
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f"user-agent={REQ_HEADERS['User-Agent']}")
    
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    try:
        driver_path = ChromeDriverManager().install()
        service = Service(driver_path)
        driver = webdriver.Chrome(service=service, options=options)
    except Exception as e:
        print(f" WebDriverã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []
        
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    driver.get(search_url)
    
    try:
        WebDriverWait(driver, 20).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "li[class*='sc-1u4589e-0']"))
        )
        time.sleep(3)
    except Exception as e:
        print(f"  âš ï¸ ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰ã¾ãŸã¯è¦ç´ æ¤œç´¢ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ã‚¨ãƒ©ãƒ¼: {e}")
        time.sleep(5)
    
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()
    
    # è¨˜äº‹ãƒªã‚¹ãƒˆã®è¦ªè¦ç´ ã‚’ç‰¹å®š (ã‚»ãƒ¬ã‚¯ã‚¿ã¯é©å®œèª¿æ•´ãŒå¿…è¦ã«ãªã‚‹å ´åˆãŒã‚ã‚‹)
    articles = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    
    articles_data = []
    today_jst = jst_now()
    
    for article in articles:
        try:
            # A. ã‚¿ã‚¤ãƒˆãƒ«
            title_tag = article.find("div", class_=re.compile("sc-3ls169-0"))
            title = title_tag.text.strip() if title_tag else ""
            
            # B. URL
            link_tag = article.find("a", href=True)
            url = link_tag["href"] if link_tag and link_tag["href"].startswith("https://news.yahoo.co.jp/articles/") else ""
            
            # C. æŠ•ç¨¿æ—¥æ™‚ (Cåˆ—) æŠ½å‡º
            date_str = ""
            time_tag = article.find("time")
            if time_tag:
                date_str = time_tag.text.strip()
            
            # D. ã‚½ãƒ¼ã‚¹ (Dåˆ—) æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã®æ”¹å–„
            source_text = ""
            source_container = article.find("div", class_=re.compile("sc-n3vj8g-0"))
            
            if source_container:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚„ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å¾Œã«ç¶šãæœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¢ã™
                time_and_comments = source_container.find("div", class_=re.compile("sc-110wjhy-8"))
                
                if time_and_comments:
                    # divå†…ã®å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã‚’å–å¾—ã—ã€æ—¥ä»˜ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã®è¦ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
                    source_candidates = [
                        span.text.strip() for span in time_and_comments.find_all("span")
                        if not span.find("svg") # ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¤ã‚³ãƒ³ã§ã¯ãªã„
                        and not re.match(r'\d{1,2}/\d{1,2}\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)\d{1,2}:\d{2}', span.text.strip()) # æ—¥ä»˜ã§ã¯ãªã„
                    ]
                    # æœ€ã‚‚é•·ã„ï¼ˆã‚½ãƒ¼ã‚¹ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ï¼‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¡ç”¨
                    if source_candidates:
                        source_text = max(source_candidates, key=len)
                        
                    # ä¸Šè¨˜ã§å–å¾—ã§ããªã„å ´åˆã€ç›´ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã‚’æ¢ã™
                    if not source_text:
                        for content in time_and_comments.contents:
                            if content.name is None and content.strip() and not re.match(r'\d{1,2}/\d{1,2}\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)\d{1,2}:\d{2}', content.strip()):
                                source_text = content.strip()
                                break
                    
            if title and url:
                formatted_date = ""
                if date_str:
                    try:
                        # å–å¾—ã—ãŸç”Ÿã®æ—¥ä»˜æ–‡å­—åˆ—ã‹ã‚‰æ—¥ä»˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                        dt_obj = parse_post_date(date_str, today_jst)
                        
                        if dt_obj:
                            # ä¿®æ­£ã—ãŸ format_datetime ã‚’ä½¿ç”¨ã—ã€yyyy/mm/dd hh:mm:ss å½¢å¼ã§æ ¼ç´
                            formatted_date = format_datetime(dt_obj)
                        else:
                            # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯æ›œæ—¥ã ã‘å‰Šé™¤ã—ãŸç”Ÿæ–‡å­—åˆ—ã‚’ãã®ã¾ã¾ä¿æŒ
                            formatted_date = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", date_str).strip()
                    except:
                        formatted_date = date_str

                articles_data.append({
                    "URL": url,
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    "æŠ•ç¨¿æ—¥æ™‚": formatted_date if formatted_date else "å–å¾—ä¸å¯",
                    "ã‚½ãƒ¼ã‚¹": source_text if source_text else "å–å¾—ä¸å¯"
                })
        except Exception as e:
            continue
            
    print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(articles_data)} ä»¶å–å¾—")
    return articles_data

# ====== è©³ç´°å–å¾—é–¢æ•° (ã€ä¿®æ­£ã€‘ã€Œ1ãƒšãƒ¼ã‚¸1ã‚»ãƒ«ã€ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™) ======
def fetch_article_body_and_comments(base_url: str) -> Tuple[List[str], int, Optional[str]]:
    """
    ã€ä¿®æ­£ã€‘
    è¤‡æ•°ãƒšãƒ¼ã‚¸å·¡å›ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ã„ã€æœ¬æ–‡ã‚’ã€Œ1ãƒšãƒ¼ã‚¸1ã‚»ãƒ«ã€ã®ãƒªã‚¹ãƒˆ (10è¦ç´ ) ã¨ã—ã¦è¿”ã™ã€‚
    Returns:
        Tuple[List[str], int, Optional[str]]: 
            ( [æœ¬æ–‡P1, æœ¬æ–‡P2, ...], ã‚³ãƒ¡ãƒ³ãƒˆæ•°, æ—¥ä»˜æ–‡å­—åˆ— )
    """
    comment_count = -1 # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã¯1ãƒšãƒ¼ã‚¸ç›®ã§ã®ã¿å–å¾—
    extracted_date_str = None # æ—¥æ™‚ã‚‚1ãƒšãƒ¼ã‚¸ç›®ã§ã®ã¿å–å¾—
    
    # æˆ»ã‚Šå€¤ç”¨ã®ãƒªã‚¹ãƒˆ (10åˆ—åˆ†)
    MAX_BODY_PAGES = 10
    body_pages_list = []
    
    # URLã‹ã‚‰è¨˜äº‹IDã‚’å–å¾—
    article_id_match = re.search(r'/articles/([a-f0-9]+)', base_url)
    if not article_id_match:
        print(f"  âŒ URLã‹ã‚‰è¨˜äº‹IDãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {base_url}")
        return ["æœ¬æ–‡å–å¾—ä¸å¯"] * MAX_BODY_PAGES, -1, None
    
    # ãƒ™ãƒ¼ã‚¹URLã‹ã‚‰ ? ä»¥é™ã‚’å‰Šé™¤
    clean_base_url = base_url.split('?')[0]
    
    # æœ€å¤§10ãƒšãƒ¼ã‚¸ã®ãƒ«ãƒ¼ãƒ— (1ã‹ã‚‰10ã¾ã§)
    for page_num in range(1, MAX_BODY_PAGES + 1):
        
        # 1ãƒšãƒ¼ã‚¸ç›®ã®ã¿URLãŒç•°ãªã‚‹
        if page_num == 1:
            current_url = clean_base_url
        else:
            current_url = f"{clean_base_url}?page={page_num}"
            
        # 2. HTMLå–å¾—
        response = request_with_retry(current_url) # æ—¢å­˜ã®ãƒªãƒˆãƒ©ã‚¤é–¢æ•° ã‚’ä½¿ç”¨
        
        # ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„ (404ãªã©) or å–å¾—å¤±æ•—
        if not response:
            if page_num == 1:
                # 1ãƒšãƒ¼ã‚¸ç›®ã‹ã‚‰å¤±æ•—ã—ãŸã‚‰å³çµ‚äº†
                print(f"  âŒ è¨˜äº‹æœ¬æ–‡(1ãƒšãƒ¼ã‚¸ç›®)ã®å–å¾—ã«å¤±æ•—ã€‚: {current_url}")
                body_pages_list.append("æœ¬æ–‡å–å¾—ä¸å¯")
            else:
                # 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™ã®å¤±æ•—ã¯ã€ãã‚ŒãŒæœ€çµ‚ãƒšãƒ¼ã‚¸ã ã£ãŸã¨ã„ã†ã“ã¨
                print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ {page_num} ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸã€‚æœ¬æ–‡å–å¾—ã‚’å®Œäº†ã—ã¾ã™ã€‚")
            break # ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
        
        print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ {page_num} ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        soup = BeautifulSoup(response.text, 'html.parser')

        # 3. è¨˜äº‹æœ¬æ–‡ã®æŠ½å‡º
        article_content = soup.find('article') or soup.find('div', class_='article_body') or soup.find('div', class_=re.compile(r'article_detail|article_body'))
        
        current_page_body_parts = []
        if article_content:
            # æœ€æ–°ã®HTMLæ§‹é€ ã«å¯¾å¿œã—ãŸã‚»ãƒ¬ã‚¯ã‚¿
            paragraphs = article_content.find_all('p', class_=re.compile(r'sc-\w+-0\s+\w+.*highLightSearchTarget'))
            if not paragraphs: # ä¸Šè¨˜ã‚»ãƒ¬ã‚¯ã‚¿ã§å–å¾—ã§ããªã‘ã‚Œã°æ±ç”¨<p>ã‚’è©¦ã™
                paragraphs = article_content.find_all('p')
            
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text:
                    current_page_body_parts.append(text)
        
        # 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™ã§æœ¬æ–‡ãŒå–ã‚Œãªã‹ã£ãŸå ´åˆã€ãã“ã§çµ‚äº†
        if not current_page_body_parts and page_num > 1:
             print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ {page_num} ã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å–å¾—ã‚’å®Œäº†ã—ã¾ã™ã€‚")
             break
        
        # å–å¾—ã—ãŸæœ¬æ–‡ã‚’çµåˆã—ã¦ã€1ã‚»ãƒ«ï¼ˆ1ãƒšãƒ¼ã‚¸ï¼‰åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã«ã™ã‚‹
        current_page_body_text = "\n".join(current_page_body_parts)
        
        if not current_page_body_text and page_num == 1:
            body_pages_list.append("æœ¬æ–‡å–å¾—ä¸å¯")
        elif not current_page_body_text:
            pass # 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™ã§ç©ºã®å ´åˆã¯ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ãªã„
        else:
            body_pages_list.append(current_page_body_text)

        
        # --- ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã¨æ—¥æ™‚ã¯ 1ãƒšãƒ¼ã‚¸ç›®ã‹ã‚‰ã®ã¿å–å¾— ---
        if page_num == 1:
            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°
            comment_button = soup.find("button", attrs={"data-cl-params": re.compile(r"cmtmod")}) or \
                                 soup.find("a", attrs={"data-cl-params": re.compile(r"cmtmod")})
            if comment_button:
                text = comment_button.get_text(strip=True).replace(",", "")
                match = re.search(r'(\d+)', text)
                if match:
                    comment_count = int(match.group(1))

            # æ—¥æ™‚ (1ãƒšãƒ¼ã‚¸ç›®ã®æœ¬æ–‡ã®å†’é ­ã‹ã‚‰)
            body_text_partial_for_date = "\n".join(current_page_body_parts[:3]) # 1ãƒšãƒ¼ã‚¸ç›®ã®æœ¬æ–‡ã®å†’é ­3è¡Œ
            if body_text_partial_for_date:
                match_date = re.search(r'(\d{1,2}/\d{1,2})\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)(\s*)(\d{1,2}:\d{2})é…ä¿¡', body_text_partial_for_date)
                if match_date:
                    month_day = match_date.group(1)
                    time_str = match_date.group(3)
                    extracted_date_str = f"{month_day} {time_str}"
                    
        # ãƒšãƒ¼ã‚¸ãŒåˆ‡ã‚Šæ›¿ã‚ã‚‹é–“ã«å°‘ã—å¾…æ©Ÿ (è² è·è»½æ¸›)
        if page_num < MAX_BODY_PAGES:
            time.sleep(0.5 + random.random() * 0.5) 

    # 4. 10åˆ—ã«æº€ãŸãªã„å ´åˆã€æ®‹ã‚Šã‚’ã€Œ-ã€ã§åŸ‹ã‚ã‚‹
    if len(body_pages_list) < MAX_BODY_PAGES:
        body_pages_list.extend(["-"] * (MAX_BODY_PAGES - len(body_pages_list)))
    
    # 10åˆ—ã‚’è¶…ãˆãŸå ´åˆã¯ã‚¹ãƒ©ã‚¤ã‚¹ã™ã‚‹ (ã»ã¼ç™ºç”Ÿã—ãªã„ãŒå¿µã®ãŸã‚)
    final_body_pages_list = body_pages_list[:MAX_BODY_PAGES]
    
    return final_body_pages_list, comment_count, extracted_date_str


# ====== ã€æ–°è¦è¿½åŠ ã€‘ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡å–å¾—é–¢æ•° (æœ€å¤§10ãƒšãƒ¼ã‚¸ + å­˜åœ¨ç¢ºèª) ======
def fetch_comments_by_page(base_url: str, total_comment_count: int) -> List[str]:
    """
    è¨˜äº‹ã®ã‚³ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸ã‚’å·¡å›ã—ã€ã€Œ1ãƒšãƒ¼ã‚¸1ã‚»ãƒ«ã€ã®å½¢å¼ã§ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã‚’å–å¾—ã™ã‚‹ã€‚
    
    Args:
        base_url (str): /articles/ ã‹ã‚‰å§‹ã¾ã‚‹è¨˜äº‹URL
        total_comment_count (int): Fåˆ—ã«è¡¨ç¤ºã•ã‚Œã‚‹ã‚³ãƒ¡ãƒ³ãƒˆç·æ•° (100ä»¶ã‚’è¶…ãˆã‚‹ã‹åˆ¤å®šã«ä½¿ç”¨)

    Returns:
        List[str]: 
            11å€‹ã®è¦ç´ ã‚’æŒã¤ãƒªã‚¹ãƒˆã€‚
            [ "Page 1 comments...", "Page 2 comments...", ..., "100ä»¶ä»¥ä¸Šã‚ã‚Š" ]
    """
    
    # å–å¾—ã™ã‚‹æœ€å¤§ãƒšãƒ¼ã‚¸æ•° (Jåˆ—ï½Såˆ—ã®10åˆ—åˆ†)
    MAX_PAGES_TO_SCRAPE = 10
    
    # è¨˜äº‹URL (.../articles/...) ã‚’ã‚³ãƒ¡ãƒ³ãƒˆURL (.../comments/...) ã«å¤‰æ›
    comment_base_url = base_url.split('?')[0].replace("/articles/", "/comments/")
    if "/comments/" not in comment_base_url:
        print(f"    - âš ï¸ ã‚³ãƒ¡ãƒ³ãƒˆURLã®ç”Ÿæˆã«å¤±æ•—: {base_url}")
        # 11åˆ—åˆ†ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
        return ["ã‚³ãƒ¡ãƒ³ãƒˆURLç”Ÿæˆå¤±æ•—"] * (MAX_PAGES_TO_SCRAPE + 1)

    page_comments_list = [] # æˆ»ã‚Šå€¤ã¨ãªã‚‹ãƒªã‚¹ãƒˆ (æœ€å¤§11è¦ç´ )

    for page_num in range(1, MAX_PAGES_TO_SCRAPE + 1):
        
        # ã‚³ãƒ¡ãƒ³ãƒˆç·æ•°ãŒ 10ä»¶ (page_num=2) ã‚„ 20ä»¶ (page_num=3) ã®å ´åˆã€
        # ãã‚Œä»¥ä¸Šãƒšãƒ¼ã‚¸ã¯å­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’åœæ­¢ã™ã‚‹
        if total_comment_count <= (page_num - 1) * 10 and page_num > 1:
             print(f"    - ã‚³ãƒ¡ãƒ³ãƒˆç·æ•° ({total_comment_count}ä»¶) ã«åŸºã¥ãã€ãƒšãƒ¼ã‚¸ {page_num} ä»¥é™ã®å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã€‚")
             break # forãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹

        current_url = f"{comment_base_url}?page={page_num}"
        
        # request_with_retry ã¯æ—¢å­˜ã®é–¢æ•°ã‚’æµç”¨
        response = request_with_retry(current_url, max_retries=2) 
        
        if not response:
            print(f"    - ã‚³ãƒ¡ãƒ³ãƒˆ ãƒšãƒ¼ã‚¸ {page_num} ( {current_url} ) ãŒå­˜åœ¨ã—ãªã„ã‹å–å¾—å¤±æ•—ã€‚")
            page_comments_list.append("ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ä¸å¯")
            break # å¤±æ•—ã—ãŸã‚‰ã€ãã‚Œä»¥é™ã®ãƒšãƒ¼ã‚¸ï¼ˆä¾‹ï¼špage 3, 4...ï¼‰ã®è©¦è¡Œã¯ã—ãªã„

        print(f"    - ã‚³ãƒ¡ãƒ³ãƒˆ ãƒšãƒ¼ã‚¸ {page_num} ã‚’å–å¾—ä¸­...")
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã¯ <p data-testid="comment-text"> ã«æ ¼ç´ã•ã‚Œã¦ã„ã‚‹
        comment_tags = soup.find_all('p', attrs={"data-testid": "comment-text"})
        
        comments_on_this_page = []
        for tag in comment_tags:
            text = tag.get_text(strip=True)
            if text:
                comments_on_this_page.append(text)
        
        if not comments_on_this_page:
            # ãƒšãƒ¼ã‚¸ã¯ã‚ã‚‹ãŒã‚³ãƒ¡ãƒ³ãƒˆãŒï¼ˆä½•ã‚‰ã‹ã®ç†ç”±ã§ï¼‰ãªã„å ´åˆ
            print(f"    - ã‚³ãƒ¡ãƒ³ãƒˆ ãƒšãƒ¼ã‚¸ {page_num} ã‹ã‚‰ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            page_comments_list.append("ã‚³ãƒ¡ãƒ³ãƒˆãªã—")
            break # å¿µã®ãŸã‚ã€ã“ã‚Œä»¥ä¸Šã¯é€²ã¾ãªã„
        
        # å–å¾—ã—ãŸã‚³ãƒ¡ãƒ³ãƒˆ (æœ€å¤§10ä»¶) ã‚’æ”¹è¡Œã§çµåˆã—ã€1ã¤ã®ã‚»ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿ã«ã™ã‚‹
        page_comments_list.append("\n".join(comments_on_this_page))
        time.sleep(0.5 + random.random() * 0.5) # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›

    # --- 10ãƒšãƒ¼ã‚¸åˆ†ã®å‡¦ç†ãŒå®Œäº† ---

    # é€”ä¸­ã§ãƒ«ãƒ¼ãƒ—ãŒçµ‚äº†ã—ãŸå ´åˆï¼ˆä¾‹ï¼š3ãƒšãƒ¼ã‚¸ç›®ã§çµ‚ã‚ã£ãŸï¼‰ã€æ®‹ã‚Šã®åˆ—ï¼ˆ4ï½10ï¼‰ã‚’ã€Œ-ã€ã§åŸ‹ã‚ã‚‹
    if len(page_comments_list) < MAX_PAGES_TO_SCRAPE:
        page_comments_list.extend(["-"] * (MAX_PAGES_TO_SCRAPE - len(page_comments_list)))
    
    # Tåˆ—ï¼ˆ11åˆ—ç›®ï¼‰ã®å‡¦ç†: 11ãƒšãƒ¼ã‚¸ç›® (101ä»¶ç›®) ãŒå­˜åœ¨ã™ã‚‹ã‹ï¼Ÿ
    # Fåˆ—ã‹ã‚‰å–å¾—ã—ãŸã‚³ãƒ¡ãƒ³ãƒˆç·æ•°ãŒ 100ä»¶ (10ãƒšãƒ¼ã‚¸ * 10ä»¶) ã‚ˆã‚Šå¤šã„ã‹
    if total_comment_count > (MAX_PAGES_TO_SCRAPE * 10):
        page_comments_list.append(f"{MAX_PAGES_TO_SCRAPE * 10}ä»¶ä»¥ä¸Šã‚ã‚Š")
    else:
        page_comments_list.append("-")
        
    return page_comments_list


# ====== ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ“ä½œé–¢æ•° (ã‚½ãƒ¼ãƒˆ/ç½®æ›ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£) ======

def set_row_height(ws: gspread.Worksheet, row_height_pixels: int):
    try:
        requests = []
        requests.append({
           "updateDimensionProperties": {
                 "range": {
                     "sheetId": ws.id,
                     "dimension": "ROWS",
                     "startIndex": 1,
                     "endIndex": ws.row_count
                 },
                 "properties": {
                     "pixelSize": row_height_pixels
                 },
                 "fields": "pixelSize"
            }
        })
        ws.spreadsheet.batch_update({"requests": requests})
        print(f" 2è¡Œç›®ä»¥é™ã®**è¡Œã®é«˜ã•**ã‚’ {row_height_pixels} ãƒ”ã‚¯ã‚»ãƒ«ã«è¨­å®šã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f" âš ï¸ è¡Œé«˜è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")


def ensure_source_sheet_headers(sh: gspread.Spreadsheet) -> gspread.Worksheet:
    try:
        ws = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(title=SOURCE_SHEET_NAME, rows=str(MAX_SHEET_ROWS_FOR_REPLACE), cols=str(len(YAHOO_SHEET_HEADERS)))
        
    current_headers = ws.row_values(1)
    if current_headers != YAHOO_SHEET_HEADERS:
        ws.update(range_name=f'A1:{gspread.utils.rowcol_to_a1(1, len(YAHOO_SHEET_HEADERS))}', values=[YAHOO_SHEET_HEADERS])
    return ws

def write_news_list_to_source(gc: gspread.Client, articles: list[dict]):
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    worksheet = ensure_source_sheet_headers(sh)
            
    existing_data = worksheet.get_all_values(value_render_option='UNFORMATTED_VALUE')
    # æ—¢å­˜ã®Aåˆ—ï¼ˆURLï¼‰ã‚’ã‚»ãƒƒãƒˆã«æ ¼ç´
    existing_urls = set(str(row[0]) for row in existing_data[1:] if len(row) > 0 and str(row[0]).startswith("http"))
    
    # URLãŒé‡è¤‡ã—ãªã„æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡º
    new_data = [[a['URL'], a['ã‚¿ã‚¤ãƒˆãƒ«'], a['æŠ•ç¨¿æ—¥æ™‚'], a['ã‚½ãƒ¼ã‚¹']] for a in articles if a['URL'] not in existing_urls]
    
    if new_data:
        # Aï½Dåˆ—ã«è¿½è¨˜
        worksheet.append_rows(new_data, value_input_option='USER_ENTERED')
        print(f"  SOURCEã‚·ãƒ¼ãƒˆã« {len(new_data)} ä»¶è¿½è¨˜ã—ã¾ã—ãŸã€‚")
    else:
        print("  SOURCEã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã™ã¹ãæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

def sort_yahoo_sheet(gc: gspread.Client):
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        worksheet = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print("ã‚½ãƒ¼ãƒˆã‚¹ã‚­ãƒƒãƒ—: Yahooã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # æœ€çµ‚è¡Œã‚’å–å¾—ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ç¯„å›²ã‚’ç‰¹å®šã™ã‚‹ãŸã‚ï¼‰
    last_row = len(worksheet.col_values(1))
    
    if last_row <= 1:
        print("ã‚½ãƒ¼ãƒˆå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚½ãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    # --- ğŸš¨ æ›œæ—¥å‰Šé™¤ã®ãŸã‚ã® batch_update (æ—¢å­˜) ---
    try:
        requests = []
        
        # æ›œæ—¥ãƒªã‚¹ãƒˆ
        days_of_week = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
        
        # 1. å„æ›œæ—¥ã«å¯¾å¿œã™ã‚‹å€‹åˆ¥ã®ç½®æ›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç”Ÿæˆ (7ã¤ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ)
        for day in days_of_week:
            requests.append({
                "findReplace": {
                    "range": {
                        "sheetId": worksheet.id,
                        "startRowIndex": 1, # 2è¡Œç›®ã‹ã‚‰
                        "endRowIndex": MAX_SHEET_ROWS_FOR_REPLACE, # 10000è¡Œç›®ã¾ã§
                        "startColumnIndex": 2, # Cåˆ—
                        "endColumnIndex": 3 # Cåˆ—
                    },
                    "find": rf"\({day}\)", # f-stringã¨raw stringã§ \(æœˆ\) ã®æ­£è¦è¡¨ç¾ã‚’ç”Ÿæˆ
                    "replacement": "",
                    "searchByRegex": True,
                }
            })
            
        # 2. æ›œæ—¥ã®ç›´å¾Œã«æ®‹ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚„é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã€åŠè§’ã‚¹ãƒšãƒ¼ã‚¹1ã¤ã«çµ±ä¸€ (1ã¤ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ)
        requests.append({
            "findReplace": {
                "range": {
                    "sheetId": worksheet.id,
                    "startRowIndex": 1,
                    "endRowIndex": MAX_SHEET_ROWS_FOR_REPLACE,
                    "startColumnIndex": 2,
                    "endColumnIndex": 3
                },
                "find": r"\s{2,}",
                "replacement": " ",
                "searchByRegex": True,
            }
        })
        
        # 3. æœ€å¾Œã«æ®‹ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹å‰å¾Œã®ä¸è¦ãªç©ºç™½ã‚’å‰Šé™¤ (Trimæ©Ÿèƒ½ã®ä»£æ›¿ - 1ã¤ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ)
        requests.append({
            "findReplace": {
                "range": {
                    "sheetId": worksheet.id,
                    "startRowIndex": 1,
                    "endRowIndex": MAX_SHEET_ROWS_FOR_REPLACE,
                    "startColumnIndex": 2,
                    "endColumnIndex": 3
                },
                "find": r"^\s+|\s+$",
                "replacement": "",
                "searchByRegex": True,
            }
        })
        
        # batch_update ã§ã¾ã¨ã‚ã¦å®Ÿè¡Œ
        worksheet.spreadsheet.batch_update({"requests": requests})
        print(" ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§Cåˆ—ã®**æ›œæ—¥è¨˜è¼‰ã‚’å€‹åˆ¥ã«å‰Šé™¤ã—ã€ä½“è£ã‚’æ•´ãˆã¾ã—ãŸ**ã€‚")
        
    except Exception as e:
        print(f" âš ï¸ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã®ç½®æ›ã‚¨ãƒ©ãƒ¼: {e}")
    # ----------------------------------------------------

    # --- ã€ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆâ‘¡ã€‘æ—¥æ™‚ã®è¡¨ç¤ºå½¢å¼å¤‰æ›´ (repeatCellã‚’ä½¿ç”¨) ---
    # --- ã€ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆã€‘æ›¸å¼è¨­å®šå¾Œã«sleepã‚’è¿½åŠ  ---
    try:
        format_requests = []
        format_requests.append({
            "repeatCell": {
                "range": {
                    "sheetId": worksheet.id,
                    "startRowIndex": 1,
                    "endRowIndex": last_row,
                    "startColumnIndex": 2,
                    "endColumnIndex": 3
                },
                "cell": {
                    "userEnteredFormat": {
                        "numberFormat": {
                            "type": "DATE_TIME",
                            "pattern": "yyyy/mm/dd hh:mm:ss"
                        }
                    }
                },
                "fields": "userEnteredFormat.numberFormat"
            }
        })
        worksheet.spreadsheet.batch_update({"requests": format_requests})
        print(f" âœ… Cåˆ—(2è¡Œç›®ã€œ{last_row}è¡Œ) ã®è¡¨ç¤ºå½¢å¼ã‚’ 'yyyy/mm/dd hh:mm:ss' ã«è¨­å®šã—ã¾ã—ãŸã€‚")
        time.sleep(2)
    except Exception as e:
        print(f" âš ï¸ Cåˆ—ã®è¡¨ç¤ºå½¢å¼è¨­å®šã‚¨ãƒ©ãƒ¼: {e}") 

    # --- Google Sheets APIã®sortRangeãƒªã‚¯ã‚¨ã‚¹ãƒˆ ---
    try:
        sort_request = {
            "sortRange": {
                "range": {
                    "sheetId": worksheet.id,
                    "startRowIndex": 1,
                    "endRowIndex": last_row,
                    "startColumnIndex": 0,
                    "endColumnIndex": len(YAHOO_SHEET_HEADERS)
                },
                "sortSpecs": [
                    {
                        "dimensionIndex": 2,
                        "sortOrder": "DESCENDING"
                    }
                ]
            }
        }
        worksheet.spreadsheet.batch_update({"requests": [sort_request]})
        print(" âœ… SOURCEã‚·ãƒ¼ãƒˆã‚’æŠ•ç¨¿æ—¥æ™‚ã®**æ–°ã—ã„é †**ã«Google Sheets APIã§ä¸¦ã³æ›¿ãˆã¾ã—ãŸã€‚")
    except Exception as e:
        print(f" âš ï¸ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã®ã‚½ãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    # --- ã€ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆâ‘ ã€‘ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§ã®ã‚½ãƒ¼ãƒˆ (APIã‚½ãƒ¼ãƒˆ) ---
    try:
        last_col_index = len(YAHOO_SHEET_HEADERS) # 31 (AEåˆ—)
        # gspread.utils.col_to_letter ã®ä»£æ›¿é–¢æ•°ã‚’ä½¿ç”¨
        last_col_a1 = gspread_util_col_to_letter(last_col_index)
        sort_range = f'A2:{last_col_a1}{last_row}'

        # Cåˆ—ï¼ˆ3åˆ—ç›®ï¼‰ã‚’é™é †ï¼ˆæ–°ã—ã„é †ï¼‰ã§ã‚½ãƒ¼ãƒˆ
        # gspreadã®sortãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        worksheet.sort((3, 'desc'), range=sort_range)
        print(" âœ… SOURCEã‚·ãƒ¼ãƒˆã‚’æŠ•ç¨¿æ—¥æ™‚ã®**æ–°ã—ã„é †**ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§ä¸¦ã³æ›¿ãˆã¾ã—ãŸã€‚")
    except Exception as e:
        print(f" âš ï¸ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã®ã‚½ãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

# ====== æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ã¨å³æ™‚æ›´æ–° (ã€ä¿®æ­£ã€‘åˆ—ã‚·ãƒ•ãƒˆå¯¾å¿œ) ======

def fetch_details_and_update_sheet(gc: gspread.Client):
    """ 
    ã€ä¿®æ­£ã€‘
    åˆ—ã®ã‚·ãƒ•ãƒˆ (E-Nåˆ—ã«æœ¬æ–‡ã€Oåˆ—ã«ã‚³ãƒ¡ãƒ³ãƒˆæ•°) ã«å¯¾å¿œã€‚
    """
    
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        ws = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print("è©³ç´°å–å¾—ã‚¹ã‚­ãƒƒãƒ—: Yahooã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
        
    all_values = ws.get_all_values(value_render_option='UNFORMATTED_VALUE')
    if len(all_values) <= 1:
        print(" Yahooã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€è©³ç´°å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
        
    data_rows = all_values[1:]
    update_count = 0
    
    print("\n===== ğŸ“„ ã‚¹ãƒ†ãƒƒãƒ—â‘¡ è¨˜äº‹æœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ãƒ»å³æ™‚åæ˜  =====")

    now_jst = jst_now()
    three_days_ago = (now_jst - timedelta(days=3)).replace(hour=0, minute=0, second=0, microsecond=0)

    for idx, data_row in enumerate(data_rows):
        if len(data_row) < len(YAHOO_SHEET_HEADERS):
            data_row.extend([''] * (len(YAHOO_SHEET_HEADERS) - len(data_row)))
            
        row_num = idx + 2
        
        url = str(data_row[0])
        title = str(data_row[1])
        post_date_raw = str(data_row[2]) # Cåˆ—
        source = str(data_row[3])        # Dåˆ—
        
        # â–¼â–¼â–¼ã€ä¿®æ­£ã€‘ Eåˆ—ï½Nåˆ— (æœ¬æ–‡) ã¨ Oåˆ— (ã‚³ãƒ¡ãƒ³ãƒˆæ•°) ã‚’èª­ã¿è¾¼ã‚€ â–¼â–¼â–¼
        body_p1 = str(data_row[4])       # Eåˆ— (æœ¬æ–‡P1)
        comment_count_str = str(data_row[14]) # Oåˆ— (ã‚³ãƒ¡ãƒ³ãƒˆæ•°)
        
        if not url.strip() or not url.startswith('http'):
            print(f"  - è¡Œ {row_num}: URLãŒç„¡åŠ¹ãªãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            continue

        is_content_fetched = (body_p1.strip() and body_p1 != "æœ¬æ–‡å–å¾—ä¸å¯") # æœ¬æ–‡P1ãŒå–å¾—æ¸ˆã¿ã‹
        needs_body_fetch = not is_content_fetched # æœ¬æ–‡å–å¾—ãŒåˆå›å¿…è¦ã‹ã©ã†ã‹
        
        post_date_dt = parse_post_date(post_date_raw, now_jst)
        is_within_three_days = (post_date_dt and post_date_dt >= three_days_ago)
        
        # --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ (å¤‰æ›´ãªã—) ---
        if is_content_fetched and not is_within_three_days:
            print(f"  - è¡Œ {row_num} (è¨˜äº‹: {title[:20]}...): æœ¬æ–‡å–å¾—æ¸ˆã¿ã‹ã¤3æ—¥ã‚ˆã‚Šå¤ã„è¨˜äº‹ã®ãŸã‚ã€**å®Œå…¨ã‚¹ã‚­ãƒƒãƒ—**ã€‚")
            continue
        is_comment_only_update = is_content_fetched and is_within_three_days
        needs_full_fetch = needs_body_fetch
        needs_detail_fetch = is_comment_only_update or needs_full_fetch

        if not needs_detail_fetch:
            print(f"  - è¡Œ {row_num} (è¨˜äº‹: {title[:20]}...): è©³ç´°æ›´æ–°ã®å¿…è¦ãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            continue

        # --- è©³ç´°å–å¾—ã‚’å®Ÿè¡Œ ---
        if needs_full_fetch:
            print(f"  - è¡Œ {row_num} (è¨˜äº‹: {title[:20]}...): **æœ¬æ–‡(P1-P10)/ã‚³ãƒ¡ãƒ³ãƒˆæ•°/æ—¥æ™‚è£œå®Œ/ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ ã‚’å–å¾—ä¸­... (å®Œå…¨å–å¾—)**")
        elif is_comment_only_update:
            print(f"  - è¡Œ {row_num} (è¨˜äº‹: {title[:20]}...): **ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’æ›´æ–°ä¸­... (è»½é‡æ›´æ–°)**")
            
        # â–¼â–¼â–¼ã€ä¿®æ­£ã€‘ fetch_article_body_and_comments ã¯ 10è¦ç´ ã®List[str]ã‚’è¿”ã™ â–¼â–¼â–¼
        fetched_body_pages, fetched_comment_count, extracted_date = fetch_article_body_and_comments(url)

        fetched_comments_list = None
        if needs_full_fetch:
            try:
                comment_count_for_check = int(fetched_comment_count)
            except ValueError:
                comment_count_for_check = -1 
            fetched_comments_list = fetch_comments_by_page(url, total_comment_count=comment_count_for_check) 
        # â–²â–²â–²ã€ä¿®æ­£ã€‘ fetch_article_body_and_comments ã¯ 10è¦ç´ ã®List[str]ã‚’è¿”ã™ â–²â–²â–²

        new_comment_count = comment_count_str
        new_post_date = post_date_raw
        
        needs_cd_update = False # C, Dåˆ—ã®æ›´æ–°ãƒ•ãƒ©ã‚°
        needs_en_update = False # E-Nåˆ— (æœ¬æ–‡) ã®æ›´æ–°ãƒ•ãƒ©ã‚°
        needs_o_update = False  # Oåˆ— (ã‚³ãƒ¡ãƒ³ãƒˆæ•°) ã®æ›´æ–°ãƒ•ãƒ©ã‚°

        # 1. E-Nåˆ—(æœ¬æ–‡)ã®æ›´æ–° (æœ¬æ–‡æœªå–å¾—ã®å ´åˆã®ã¿)
        if needs_full_fetch:
            # fetched_body_pages[0] (P1) ã¨ data_row[4] (Eåˆ—) ã‚’æ¯”è¼ƒ
            if fetched_body_pages[0] != "æœ¬æ–‡å–å¾—ä¸å¯" and fetched_body_pages[0] != str(data_row[4]):
                needs_en_update = True
            elif fetched_body_pages[0] == "æœ¬æ–‡å–å¾—ä¸å¯" and str(data_row[4]) != "æœ¬æ–‡å–å¾—ä¸å¯":
                 needs_en_update = True # æœ¬æ–‡å–å¾—ä¸å¯ã«ãªã£ãŸå ´åˆ
        elif is_comment_only_update and fetched_body_pages[0] == "æœ¬æ–‡å–å¾—ä¸å¯" and str(data_row[4]) != "æœ¬æ–‡å–å¾—ä¸å¯":
             # ã‚³ãƒ¡ãƒ³ãƒˆæ›´æ–°ç›®çš„ã§å©ã„ãŸãŒ404ã«ãªã£ã¦ã„ãŸå ´åˆ
             needs_en_update = True
            
        # 2. Cåˆ—(æ—¥æ™‚)ã®æ›´æ–°
        if needs_full_fetch and ("å–å¾—ä¸å¯" in post_date_raw or not post_date_raw.strip()) and extracted_date:
            dt_obj = parse_post_date(extracted_date, now_jst)
            if dt_obj:
                formatted_dt = format_datetime(dt_obj)
                if formatted_dt != post_date_raw:
                    new_post_date = formatted_dt
                    needs_cd_update = True
            else:
                raw_date = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)$", "", extracted_date).strip()
                if raw_date != post_date_raw:
                    new_post_date = raw_date
                    needs_cd_update = True
            
        # 3. Oåˆ—(ã‚³ãƒ¡ãƒ³ãƒˆæ•°)ã®æ›´æ–°
        if fetched_comment_count != -1:
            if needs_full_fetch or is_comment_only_update:
                if str(fetched_comment_count) != comment_count_str:
                    new_comment_count = str(fetched_comment_count)
                    needs_o_update = True
        else:
            if needs_detail_fetch: 
                print(f"    - âš ï¸ ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ—¢å­˜ã®å€¤ ({comment_count_str}) ã‚’ç¶­æŒã—ã¾ã™ã€‚")

        # â–¼â–¼â–¼ã€ä¿®æ­£ã€‘ æ›´æ–°å‡¦ç†ã‚’åˆ—ç¯„å›²ã”ã¨ã«åˆ†å‰² â–¼â–¼â–¼
        if needs_cd_update:
            # C, Dåˆ—ã‚’æ›´æ–° (Dåˆ—ã¯ã‚½ãƒ¼ã‚¹ã€‚å¤‰æ›´ãªã—ã ãŒç¯„å›²ã«å«ã‚ã‚‹)
            ws.update(
                range_name=f'C{row_num}:D{row_num}',
                values=[[new_post_date, source]],
                value_input_option='USER_ENTERED'
            )
            update_count += 1
            time.sleep(1)

        if needs_en_update:
            # E-Nåˆ— (æœ¬æ–‡P1-P10) ã‚’æ›´æ–°
            ws.update(
                range_name=f'E{row_num}:N{row_num}',
                values=[fetched_body_pages], # 1x10 ã®ãƒªã‚¹ãƒˆ
                value_input_option='USER_ENTERED'
            )
            update_count += 1
            time.sleep(1)
            
        if needs_o_update:
            # Oåˆ— (ã‚³ãƒ¡ãƒ³ãƒˆæ•°) ã‚’æ›´æ–°
            ws.update(
                range_name=f'O{row_num}',
                values=[[new_comment_count]],
                value_input_option='USER_ENTERED'
            )
            update_count += 1
            time.sleep(1)

        if fetched_comments_list:
            # Såˆ—ï½ACåˆ— (ã‚³ãƒ¡ãƒ³ãƒˆP1ï½P11ä»¥é™) ã®11åˆ—ã«æ›¸ãè¾¼ã‚€
            ws.update(
                range_name=f'S{row_num}:AC{row_num}',
                values=[fetched_comments_list], # 1x11 ã®ãƒªã‚¹ãƒˆ
                value_input_option='USER_ENTERED'
            )
            print(f"    - âœ… ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ (Såˆ—ï½ACåˆ—) ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")
            time.sleep(0.5)
        # â–²â–²â–²ã€ä¿®æ­£ã€‘ æ›´æ–°å‡¦ç†ã‚’åˆ—ç¯„å›²ã”ã¨ã«åˆ†å‰² â–²â–²â–²

# ====== Geminiåˆ†æã®å®Ÿè¡Œã¨å¼·åˆ¶ä¸­æ–­ (ã€ä¿®æ­£ã€‘åˆ—ã‚·ãƒ•ãƒˆãƒ»æ¶ˆè²»é‡å¯¾ç­–) ======

def analyze_with_gemini_and_update_sheet(gc: gspread.Client):
    """ 
    ã€ä¿®æ­£ã€‘
    åˆ—ã®ã‚·ãƒ•ãƒˆã«å¯¾å¿œã€‚
    - E-Nåˆ— (P1-P10) ã‹ã‚‰æœ¬æ–‡ã‚’èª­ã¿å–ã‚Šã€çµåˆã—ã¦Geminiã«æ¸¡ã™ã€‚
    - P-Råˆ— (åŸºæœ¬åˆ†æ) ã«æ›¸ãè¾¼ã‚€ã€‚
    - AD-AEåˆ— (æ—¥ç”£åˆ†æ) ã«æ›¸ãè¾¼ã‚€ã€‚
    
    ã€æ¶ˆè²»é‡å¯¾ç­–ã€‘
    - 1å›ã®å®Ÿè¡Œã§åˆ†æã™ã‚‹æœ€å¤§ä»¶æ•°ã‚’ MAX_ANALYSIS_PER_RUN ã§åˆ¶é™ã™ã‚‹ã€‚
    """
    
    # â–¼â–¼â–¼ã€æ¶ˆè²»é‡å‰Šæ¸›å¯¾ç­–ã€‘â–¼â–¼â–¼
    # 1å›ã®å®Ÿè¡Œã§GeminiãŒåˆ†æã™ã‚‹è¨˜äº‹ã®æœ€å¤§æ•°ã‚’è¨­å®š (ä¾‹: 30ä»¶)
    # ã‚·ãƒ¼ãƒˆã¯æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®å‰æ
    MAX_ANALYSIS_PER_RUN = 30
    # â–²â–²â–²ã€æ¶ˆè²»é‡å‰Šæ¸›å¯¾ç­–ã€‘â–²â–²â–²

    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        ws = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        print("Geminiåˆ†æã‚¹ã‚­ãƒƒãƒ—: Yahooã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
        
    all_values = ws.get_all_values(value_render_option='UNFORMATTED_VALUE')
    if len(all_values) <= 1:
        print(" Yahooã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€Geminiåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
        
    data_rows = all_values[1:]
    update_count = 0
    
    # â–¼â–¼â–¼ã€æ¶ˆè²»é‡å‰Šæ¸›å¯¾ç­–ã€‘â–¼â–¼â–¼
    print(f"\n===== ğŸ§  ã‚¹ãƒ†ãƒƒãƒ—â‘£ Geminiåˆ†æã®å®Ÿè¡Œãƒ»å³æ™‚åæ˜  (P-R, AD-AEåˆ—) [æœ€å¤§{MAX_ANALYSIS_PER_RUN}ä»¶] =====")
    # â–²â–²â–²ã€æ¶ˆè²»é‡å‰Šæ¸›å¯¾ç­–ã€‘â–²â–²â–²

    for idx, data_row in enumerate(data_rows):

        # â–¼â–¼â–¼ã€æ¶ˆè²»é‡å‰Šæ¸›å¯¾ç­–ã€‘â–¼â–¼â–¼
        # update_count (å®Ÿéš›ã«åˆ†æãƒ»æ›´æ–°ã—ãŸä»¶æ•°) ãŒä¸Šé™ã«é”ã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
        if update_count >= MAX_ANALYSIS_PER_RUN:
            print(f"  - åˆ†æä»¶æ•°ãŒä¸Šé™ ({MAX_ANALYSIS_PER_RUN}ä»¶) ã«é”ã—ãŸãŸã‚ã€Geminiåˆ†æã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break # forãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
        # â–²â–²â–²ã€æ¶ˆè²»é‡å‰Šæ¸›å¯¾ç­–ã€‘â–²â–²â–²

        if len(data_row) < len(YAHOO_SHEET_HEADERS):
            data_row.extend([''] * (len(YAHOO_SHEET_HEADERS) - len(data_row)))
            
        row_num = idx + 2
        
        url = str(data_row[0])
        title = str(data_row[1])
        
        # â–¼â–¼â–¼ã€ä¿®æ­£ã€‘ E-Nåˆ— (4ï½13) ã‹ã‚‰æœ¬æ–‡ã‚’èª­ã¿è¾¼ã¿ã€çµåˆã™ã‚‹ â–¼â–¼â–¼
        body_pages = data_row[4:14] # Eåˆ—(index 4) ã‹ã‚‰ Nåˆ—(index 13) ã¾ã§ã®10è¦ç´ 
        body = "\n".join(str(p) for p in body_pages if str(p) and str(p) != "-") # ã€Œ-ã€ã‚’é™¤å¤–ã—ã¦çµåˆ
        
        # P-Råˆ— (AIåˆ†æ)
        company_info = str(data_row[15]) # Påˆ— (16ç•ªç›®)
        category = str(data_row[16])     # Qåˆ—
        sentiment = str(data_row[17])    # Råˆ—

        # AD-AEåˆ— (æ—¥ç”£åˆ†æ)
        nissan_mention = str(data_row[29]) # ADåˆ— (30ç•ªç›®)
        nissan_sentiment = str(data_row[30])# AEåˆ— (31ç•ªç›®)

        needs_analysis = not company_info.strip() or not category.strip() or not sentiment.strip() or \
                         not nissan_mention.strip() or not nissan_sentiment.strip()
        # â–²â–²â–²ã€ä¿®æ­£ã€‘ E-Nåˆ— (4ï½13) ã‹ã‚‰æœ¬æ–‡ã‚’èª­ã¿è¾¼ã¿ã€çµåˆã™ã‚‹ â–²â–²â–²

        if not needs_analysis:
            continue
            
        if not body.strip() or body == "æœ¬æ–‡å–å¾—ä¸å¯":
            print(f"  - è¡Œ {row_num}: æœ¬æ–‡ãŒãªã„ãŸã‚åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€N/Aã‚’è¨­å®šã€‚")
            
            # â–¼â–¼â–¼ã€ä¿®æ­£ã€‘ P-Råˆ— ã¨ AD-AEåˆ— ã‚’ N/A ã§åŸ‹ã‚ã‚‹ â–¼â–¼â–¼
            ws.update(
                range_name=f'P{row_num}:R{row_num}',
                values=[['N/A(No Body)', 'N/A', 'N/A']],
                value_input_option='USER_ENTERED'
            )
            ws.update(
                range_name=f'AD{row_num}:AE{row_num}',
                values=[['N/A(No Body)', 'N/A']],
                value_input_option='USER_ENTERED'
            )
            # â–²â–²â–²ã€ä¿®æ­£ã€‘ P-Råˆ— ã¨ AD-AEåˆ— ã‚’ N/A ã§åŸ‹ã‚ã‚‹ â–²â–²â–²
            
            update_count += 1 # N/Aè¨­å®šã‚‚ã€Œ1ä»¶å‡¦ç†ã€ã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹
            time.sleep(1)
            continue
            
        if not url.strip():
            print(f"  - è¡Œ {row_num}: URLãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            continue

        # â–¼â–¼â–¼ã€æ¶ˆè²»é‡å‰Šæ¸›å¯¾ç­–ã€‘â–¼â–¼â–¼
        print(f"  - è¡Œ {row_num} (è¨˜äº‹: {title[:20]}...): Geminiåˆ†æã‚’å®Ÿè¡Œä¸­... ({update_count + 1}/{MAX_ANALYSIS_PER_RUN}ä»¶ç›®)")
        # â–²â–²â–²ã€æ¶ˆè²»é‡å‰Šæ¸›å¯¾ç­–ã€‘â–²â–²â–²

        # --- Geminiåˆ†æã‚’å®Ÿè¡Œ (5ã¤ã®æˆ»ã‚Šå€¤) ---
        final_company_info, final_category, final_sentiment, \
        final_nissan_mention, final_nissan_sentiment, _ = analyze_with_gemini(body)
        
        # â–¼â–¼â–¼ã€ä¿®æ­£ã€‘ P-Råˆ— ã¨ AD-AEåˆ— ã«æ›¸ãè¾¼ã‚€ â–¼â–¼â–¼
        ws.update(
            range_name=f'P{row_num}:R{row_num}',
            values=[[final_company_info, final_category, final_sentiment]],
            value_input_option='USER_ENTERED'
        )
        ws.update(
            range_name=f'AD{row_num}:AE{row_num}',
            values=[[final_nissan_mention, final_nissan_sentiment]],
            value_input_option='USER_ENTERED'
        )
        # â–²â–²â–²ã€ä¿®æ­£ã€‘ P-Råˆ— ã¨ AD-AEåˆ— ã«æ›¸ãè¾¼ã‚€ â–¼â–²â–²
        
        update_count += 1 # APIå‘¼ã³å‡ºã—ã‚’ã€Œ1ä»¶å‡¦ç†ã€ã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
        time.sleep(1 + random.random() * 0.5)

    print(f" âœ… Geminiåˆ†æã‚’ {update_count} è¡Œã«ã¤ã„ã¦å®Ÿè¡Œã—ã€å³æ™‚åæ˜ ã—ã¾ã—ãŸã€‚")


# ====== ãƒ¡ã‚¤ãƒ³å‡¦ç† (å¤‰æ›´ãªã—) ======

def main():
    print("--- çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹ ---")
    
    keywords = load_keywords(KEYWORD_FILE)
    if not keywords:
        sys.exit(0)

    try:
        gc = build_gspread_client()
    except RuntimeError as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)
    
    # â‘  ã‚¹ãƒ†ãƒƒãƒ—â‘  ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—: Aï½Dåˆ—ã®å–å¾—ãƒ»è¿½è¨˜ã‚’å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
    for current_keyword in keywords:
        print(f"\n===== ğŸ”‘ ã‚¹ãƒ†ãƒƒãƒ—â‘  ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆå–å¾—: {current_keyword} =====")
        yahoo_news_articles = get_yahoo_news_with_selenium(current_keyword)
        write_news_list_to_source(gc, yahoo_news_articles)
        time.sleep(2) # ã‚·ãƒ¼ãƒˆã¸ã®é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹å›é¿

    # â‘¡ ã‚¹ãƒ†ãƒƒãƒ—â‘¡ æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ã¨å³æ™‚æ›´æ–° (E-N, O, S-ACåˆ—)
    fetch_details_and_update_sheet(gc)

    # â‘¢ ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ã‚½ãƒ¼ãƒˆã¨Cåˆ—ã®æ•´å½¢ãƒ»æ›¸å¼è¨­å®š
    print("\n===== ğŸ“‘ ã‚¹ãƒ†ãƒƒãƒ—â‘¢ è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ã‚½ãƒ¼ãƒˆã¨æ•´å½¢ =====")
    sort_yahoo_sheet(gc)
    
    # â‘£ ã‚¹ãƒ†ãƒƒãƒ—â‘£ Geminiåˆ†æã®å®Ÿè¡Œã¨å³æ™‚åæ˜  (P-R, AD-AEåˆ—)
    analyze_with_gemini_and_update_sheet(gc)
    
    print("\n--- çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Œäº† ---")

if __name__ == '__main__':
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ ã—ã¦ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã‚‹ã‚ˆã†ã«ã™ã‚‹
    if os.path.dirname(os.path.abspath(__file__)) not in sys.path:
        sys.path.append(os.path.dirname(os.path.abspath(__file__)))
        
    main()
