import os
import re
import time
import json
import gspread
import requests
import traceback
import google.generativeai as genai
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from google.oauth2.service_account import Credentials
from google.api_core.exceptions import GoogleAPIError
from gspread.exceptions import APIError as GSpreadAPIError

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° ---
# Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã‚¹ã‚³ãƒ¼ãƒ—ã¨èªè¨¼æƒ…å ±
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]
# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚­ãƒ¼ã‚’å–å¾—
SPREADSHEET_KEY = os.environ.get("SPREADSHEET_KEY")
if not SPREADSHEET_KEY:
    print("âŒ ç’°å¢ƒå¤‰æ•° 'SPREADSHEET_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    exit()

# Geminiãƒ¢ãƒ‡ãƒ«ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
gemini_model = None

# æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
SEARCH_KEYWORDS = [
    "ãƒˆãƒ¨ã‚¿", "æ—¥ç”£", "ãƒ›ãƒ³ãƒ€", "ä¸‰è±è‡ªå‹•è»Š",
    "ãƒãƒ„ãƒ€", "ã‚¹ãƒãƒ«", "ãƒ€ã‚¤ãƒãƒ„", "ã‚¹ã‚ºã‚­"
]

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
PROMPT_FILES = {
    "role": "prompt_gemini_role.txt",
    "sentiment": "prompt_posinega.txt",
    "category": "prompt_category.txt",
    "company_info": "prompt_target_company.txt",
    "nissan_mention": "prompt_nissan_mention.txt",
    "nissan_sentiment": "prompt_nissan_sentiment.txt",
}

# èª­ã¿è¾¼ã‚“ã ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ ¼ç´ã™ã‚‹è¾æ›¸
PROMPTS = {}


def setup_gspread():
    """
    Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ API ã¸ã®èªè¨¼ã‚’è¡Œã†ã€‚
    ç’°å¢ƒå¤‰æ•° GCP_SERVICE_ACCOUNT_KEY ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€ã€‚
    """
    try:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã®JSONæ–‡å­—åˆ—ã‚’å–å¾—
        creds_json_str = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
        if not creds_json_str:
            print("âŒ ç’°å¢ƒå¤‰æ•° 'GCP_SERVICE_ACCOUNT_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return None

        # JSONæ–‡å­—åˆ—ã‚’è¾æ›¸ã«å¤‰æ›
        creds_dict = json.loads(creds_json_str)

        # è¾æ›¸ã‹ã‚‰èªè¨¼æƒ…å ±ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        credentials = Credentials.from_service_account_info(creds_dict, scopes=SCOPES)
        
        # gspread ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’èªè¨¼
        gc = gspread.authorize(credentials)
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãŒé–‹ã‘ã‚‹ã‹ãƒ†ã‚¹ãƒˆ
        gc.open_by_key(SPREADSHEET_KEY)
        
        print("âœ… Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®èªè¨¼ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return gc

    except json.JSONDecodeError:
        print("âŒ 'GCP_SERVICE_ACCOUNT_KEY' ã®JSONå½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚")
        return None
    except Exception as e:
        print(f"âŒ Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None


def get_worksheet(gc, sheet_name):
    """
    gspread ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã‚·ãƒ¼ãƒˆåã‚’å—ã‘å–ã‚Šã€ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™ã€‚
    """
    if not gc:
        print(f"  âŒ ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã‚’å–å¾—ã§ãã¾ã›ã‚“ (gspreadã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæœªåˆæœŸåŒ–)ã€‚")
        return None
    try:
        spreadsheet = gc.open_by_key(SPREADSHEET_KEY)
        worksheet = spreadsheet.worksheet(sheet_name)
        return worksheet
    except GSpreadAPIError as e:
        print(f"  âŒ ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ '{sheet_name}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“: {e}")
        return None
    except Exception as e:
        print(f"  âŒ ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã®å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def load_existing_urls(ws):
    """
    SOURCE ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‹ã‚‰ B åˆ—ï¼ˆURLï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€
    é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®ã‚»ãƒƒãƒˆã¨ã—ã¦è¿”ã™ã€‚
    """
    try:
        # Båˆ—ã®å…¨ã¦ã®å€¤ã‚’å–å¾—
        urls = ws.col_values(2) # Båˆ—ã¯ 2
        # 1è¡Œç›®ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ï¼‰ã‚’é™¤ã
        return set(urls[1:])
    except Exception as e:
        print(f"  âŒ æ—¢å­˜URLã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        # ç©ºã®ã‚»ãƒƒãƒˆã‚’è¿”ã—ã¦å‡¦ç†ã‚’ç¶šè¡Œ
        return set()


def get_yahoo_news_search_results(keyword):
    """
    æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã€
    è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã€URLã€ç™ºè¡Œå…ƒã€æŠ•ç¨¿æ™‚é–“ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    """
    print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword})...")
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        response = requests.get(search_url, headers=headers)
        response.raise_for_status() # HTTPã‚¨ãƒ©ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # æ¤œç´¢çµæœã®ã‚³ãƒ³ãƒ†ãƒŠ (æ–°ã—ã„Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ§‹é€ )
        # 'newsFeed' ã‚¯ãƒ©ã‚¹ã‚’æŒã¤ ul è¦ç´ ã‚’æ¢ã™
        search_results_container = soup.find("ul", class_="newsFeed_list")

        if not search_results_container:
            # ä»£æ›¿: 'div.NewsFeed' (å¤ã„æ§‹é€  or åˆ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³)
            search_results_container = soup.find("div", class_="NewsFeed")

        if not search_results_container:
            print("  - æ¤œç´¢çµæœã®ã‚³ãƒ³ãƒ†ãƒŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (ul.newsFeed_list or div.NewsFeed)ã€‚")
            return []

        # è¨˜äº‹è¦ç´  (li ã¾ãŸã¯ div)
        # 'newsFeed_item' ã‚¯ãƒ©ã‚¹ã‚’æŒã¤ li è¦ç´ ã‚’æ¢ã™
        articles = search_results_container.find_all("li", class_="newsFeed_item")

        if not articles:
            # ä»£æ›¿: 'div.newsFeed_item' (åˆ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³)
            articles = search_results_container.find_all("div", class_="newsFeed_item")

        if not articles:
            print("  - è¨˜äº‹è¦ç´  (li.newsFeed_item or div.newsFeed_item) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return []

        results = []
        for article in articles:
            try:
                # ã‚¿ã‚¤ãƒˆãƒ«ã¨URL
                title_tag = article.find("a", class_="newsFeed_item_link")
                if not title_tag:
                    # ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿ (ä¾‹: ã‚µãƒ ãƒã‚¤ãƒ«ãƒªãƒ³ã‚¯)
                    title_tag = article.find("a", class_=re.compile(r"thumbnail_thumbnail"))
                    
                if not title_tag or "href" not in title_tag.attrs:
                    continue # ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°ã‚„URLãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

                url = title_tag["href"]
                
                # ã‚¿ã‚¤ãƒˆãƒ«å–å¾— (
                title_text_tag = article.find("div", class_="newsFeed_item_title")
                if not title_text_tag:
                     # ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿ (aã‚¿ã‚°è‡ªèº«ã®ãƒ†ã‚­ã‚¹ãƒˆ)
                    title = title_tag.text.strip()
                else:
                    title = title_text_tag.text.strip()
                
                if not title:
                     title = "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«å–å¾—å¤±æ•—ï¼‰"


                # URLãŒè¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ãƒã‚§ãƒƒã‚¯ (https://news.yahoo.co.jp/articles/...)
                if not url.startswith("https://news.yahoo.co.jp/articles/"):
                    continue

                # ç™ºè¡Œå…ƒ (ä¾‹: 'newsFeed_item_media')
                # (æ³¨: Yahoo!ã®HTMLæ§‹é€ å¤‰æ›´ã«ã‚ˆã‚Šã€ã‚»ãƒ¬ã‚¯ã‚¿ã¯é »ç¹ã«å¤‰ã‚ã‚‹)
                source_tag = article.find("span", class_="newsFeed_item_media")
                if not source_tag:
                    source_tag = article.find("div", class_=re.compile(r"newsFeed_item_subMedia"))

                source = source_tag.text.strip() if source_tag else "ç™ºè¡Œå…ƒä¸æ˜"

                # æŠ•ç¨¿æ™‚é–“ (ä¾‹: 'newsFeed_item_date')
                time_tag = article.find("time", class_="newsFeed_item_date")
                if not time_tag:
                     time_tag = article.find("div", class_=re.compile(r"newsFeed_item_date"))
                     
                post_time_str = time_tag.text.strip() if time_tag else "æ™‚é–“ä¸æ˜"

                results.append({
                    "title": title,
                    "url": url,
                    "source": source,
                    "post_time_str": post_time_str,
                    "keyword": keyword
                })

            except Exception as e:
                print(f"  - è¨˜äº‹ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                continue
                
        print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(results)} ä»¶å–å¾—")
        return results

    except requests.exceptions.RequestException as e:
        print(f"  âŒ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: {e}")
        return []
    except Exception as e:
        print(f"  âŒ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return []


def parse_relative_time(time_str):
    """
    Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ç›¸å¯¾æ™‚é–“ï¼ˆä¾‹: '1æ™‚é–“å‰', '11/11(æœˆ) 10:00'ï¼‰ã‚’
    datetime ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã™ã‚‹ã€‚
    """
    now = datetime.now()
    
    # 1. '11/11(æœˆ) 10:00' å½¢å¼ (ä»Šå¹´)
    match = re.search(r"(\d{1,2})/(\d{1,2})\(.\) (\d{1,2}):(\d{1,2})", time_str)
    if match:
        month, day, hour, minute = map(int, match.groups())
        try:
            return now.replace(month=month, day=day, hour=hour, minute=minute, second=0, microsecond=0)
        except ValueError:
            # é–å¹´ãªã©ã§å­˜åœ¨ã—ãªã„æ—¥ä»˜ã®å ´åˆã€å»å¹´ã®æ—¥ä»˜ã¨ã—ã¦æ‰±ã†
             return now.replace(year=now.year - 1, month=month, day=day, hour=hour, minute=minute, second=0, microsecond=0)

    # 2. 'â—‹åˆ†å‰' å½¢å¼
    match = re.search(r"(\d+)åˆ†å‰", time_str)
    if match:
        minutes = int(match.group(1))
        return now - timedelta(minutes=minutes)

    # 3. 'â—‹æ™‚é–“å‰' å½¢å¼
    match = re.search(r"(\d+)æ™‚é–“å‰", time_str)
    if match:
        hours = int(match.group(1))
        return now - timedelta(hours=hours)

    # 4. 'æ˜¨æ—¥' å½¢å¼
    if "æ˜¨æ—¥" in time_str:
        match = re.search(r"(\d{1,2}):(\d{1,2})", time_str)
        day_delta = 1
        if match:
            hour, minute = map(int, match.groups())
            return (now - timedelta(days=day_delta)).replace(hour=hour, minute=minute, second=0, microsecond=0)
        else:
            return now - timedelta(days=day_delta) # æ™‚é–“ä¸æ˜ãªã‚‰0æ™‚0åˆ†

    # 5. 'â—‹æ—¥å‰' å½¢å¼ (7æ—¥ä»¥ä¸Šå‰ã¯ '11/11' å½¢å¼ã«ãªã‚‹ã¯ãšã ãŒå¿µã®ãŸã‚)
    match = re.search(r"(\d+)æ—¥å‰", time_str)
    if match:
        days = int(match.group(1))
        return now - timedelta(days=days)

    # ä¸æ˜ãªå½¢å¼
    return None


def get_article_details(article_url):
    """
    è¨˜äº‹URLã‹ã‚‰æœ¬æ–‡ï¼ˆæœ€å¤§10ãƒšãƒ¼ã‚¸ï¼‰ã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã€æ­£ç¢ºãªæŠ•ç¨¿æ—¥æ™‚ã‚’å–å¾—ã™ã‚‹ã€‚
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    article_body_parts = []
    comment_count = "0" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    full_post_time = None # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    try:
        # --- 1ãƒšãƒ¼ã‚¸ç›®ã®å–å¾— (ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã¨æ—¥æ™‚ã‚‚ã“ã“ã‹ã‚‰å–ã‚‹) ---
        response = requests.get(article_url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # ã‚³ãƒ¡ãƒ³ãƒˆæ•° (ã‚»ãƒ¬ã‚¯ã‚¿ã¯å¤‰æ›´ã®å¯èƒ½æ€§å¤§)
        comment_count_tag = soup.find("a", href=re.compile(r"/comments/"))
        if comment_count_tag:
            match = re.search(r"(\d+)", comment_count_tag.text)
            if match:
                comment_count = match.group(1)

        # æ­£ç¢ºãªæŠ•ç¨¿æ—¥æ™‚ (ã‚»ãƒ¬ã‚¯ã‚¿ã¯å¤‰æ›´ã®å¯èƒ½æ€§å¤§)
        time_tag = soup.find("time")
        if time_tag and time_tag.has_attr("datetime"):
            try:
                # ISO 8601 å½¢å¼ (ä¾‹: 2023-11-10T10:00:00.000Z)
                full_post_time = datetime.fromisoformat(time_tag["datetime"].replace("Z", "+00:00"))
            except ValueError:
                print(f"  - æ—¥æ™‚ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {time_tag['datetime']}")
                full_post_time = None

        # è¨˜äº‹æœ¬æ–‡ (1ãƒšãƒ¼ã‚¸ç›®)
        # (ã‚»ãƒ¬ã‚¯ã‚¿ã¯å¤‰æ›´ã®å¯èƒ½æ€§å¤§)
        # 'article_body' or 'articleBody'
        body_container = soup.find("div", class_=re.compile(r"articleBody"))
        
        if body_container:
            # æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆ
            body_text = body_container.get_text(separator="\n", strip=True)
            article_body_parts.append(body_text)
        else:
            print(f"  - è¨˜äº‹æœ¬æ–‡(P1)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (URL: {article_url})")
            article_body_parts.append("ï¼ˆæœ¬æ–‡å–å¾—å¤±æ•—ï¼‰")


        # --- 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™ã®å–å¾— (æœ€å¤§10ãƒšãƒ¼ã‚¸) ---
        for page_num in range(2, 11): # 2ã€œ10ãƒšãƒ¼ã‚¸
            next_page_url = f"{article_url}?page={page_num}"
            try:
                response_page = requests.get(next_page_url, headers=headers)
                
                # ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„å ´åˆ (404ãªã©)
                if response_page.status_code != 200:
                    print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ {page_num} ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸã€‚æœ¬æ–‡å–å¾—ã‚’å®Œäº†ã—ã¾ã™ã€‚")
                    break # ãƒ«ãƒ¼ãƒ—ä¸­æ–­
                
                soup_page = BeautifulSoup(response_page.text, "html.parser")
                body_container_page = soup_page.find("div", class_=re.compile(r"articleBody"))
                
                if body_container_page:
                    body_text_page = body_container_page.get_text(separator="\n", strip=True)
                    # 1ãƒšãƒ¼ã‚¸ç›®ã¨åŒã˜å†…å®¹ã‹ãƒã‚§ãƒƒã‚¯ (ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®çµ‚ç«¯åˆ¤å®š)
                    if body_text_page == article_body_parts[0]:
                         print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ {page_num} ã¯1ãƒšãƒ¼ã‚¸ç›®ã¨åŒã˜å†…å®¹ã®ãŸã‚çµ‚äº†ã—ã¾ã™ã€‚")
                         break
                    
                    print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ {page_num} ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
                    article_body_parts.append(body_text_page)
                else:
                    print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ {page_num} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    break # ã‚³ãƒ³ãƒ†ãƒŠãŒè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°çµ‚äº†
                
                time.sleep(1) # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›

            except requests.exceptions.RequestException as re_e:
                # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ä¸­ã®ã‚¨ãƒ©ãƒ¼ (404 Client Error ãªã©)
                if "404" in str(re_e):
                    print(f"  âŒ ãƒšãƒ¼ã‚¸ãªã— (404 Client Error): {next_page_url}")
                    print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ {page_num} ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸã€‚æœ¬æ–‡å–å¾—ã‚’å®Œäº†ã—ã¾ã™ã€‚")
                else:
                    print(f"  âŒ ãƒšãƒ¼ã‚¸ {page_num} å–å¾—ã‚¨ãƒ©ãƒ¼: {re_e}")
                break # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰ä¸­æ–­
            except Exception as e_page:
                print(f"  âŒ ãƒšãƒ¼ã‚¸ {page_num} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e_page}")
                break

    except requests.exceptions.RequestException as re_e:
        print(f"  âŒ è¨˜äº‹è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼ (URL: {article_url}): {re_e}")
        return ["ï¼ˆæœ¬æ–‡å–å¾—å¤±æ•—ï¼‰"] * 10, "0", None
    except Exception as e:
        print(f"  âŒ è¨˜äº‹è©³ç´°å‡¦ç†ã‚¨ãƒ©ãƒ¼ (URL: {article_url}): {e}")
        traceback.print_exc()
        return ["ï¼ˆæœ¬æ–‡å–å¾—å¤±æ•—ï¼‰"] * 10, "0", None

    # 10ä»¶ã«æº€ãŸãªã„å ´åˆã¯ã€Œ-ã€ã§åŸ‹ã‚ã‚‹
    if len(article_body_parts) < 10:
        article_body_parts.extend(["-"] * (10 - len(article_body_parts)))
    
    return article_body_parts[:10], comment_count, full_post_time


def load_prompts():
    """
    ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° PROMPT_FILES ã«åŸºã¥ã„ã¦ã€
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° PROMPTS ã«æ ¼ç´ã™ã‚‹ã€‚
    """
    global PROMPTS
    print("  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
    try:
        for key, file_path in PROMPT_FILES.items():
            if not os.path.exists(file_path):
                print(f"  âŒ è­¦å‘Š: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{file_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                continue
                
            with open(file_path, "r", encoding="utf-8") as f:
                PROMPTS[key] = f.read()
        
        if not PROMPTS:
             print("  âŒ ã‚¨ãƒ©ãƒ¼: èª­ã¿è¾¼ã‚ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒ1ã¤ã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚")
             return False
             
        print("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        return True

    except Exception as e:
        print(f"  âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def initialize_gemini():
    """
    Gemini API ã‚’åˆæœŸåŒ–ã™ã‚‹ã€‚
    """
    global gemini_model
    try:
        api_key = os.environ.get("GOOGLE_API_KEY")
        if not api_key:
            print("  âŒ è­¦å‘Š: ç’°å¢ƒå¤‰æ•° 'GOOGLE_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return

        # --- (ä¿®æ­£ç®‡æ‰€) ---
        # 'module 'google.genai' has no attribute 'configure'' ã‚¨ãƒ©ãƒ¼å¯¾ç­–
        # å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’è€ƒæ…®ã—ã€configure ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if hasattr(genai, "configure"):
             genai.configure(api_key=api_key)
        else:
             print("  âš ï¸ è­¦å‘Š: genai.configure ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã®æ‰‹å‹•è¨­å®šã‚’è©¦ã¿ã¾ã™ã€‚")
             # (æ³¨: å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯ã“ã‚Œã§ã¯ä¸ååˆ†ã‹ã‚‚ã—ã‚Œãªã„ãŒã€
             #  main.yml ã§ã® --no-cache-dir --upgrade ãŒæœ¬å‘½ã®å¯¾ç­–)
             pass # APIã‚­ãƒ¼ã¯ Model() ã«æ¸¡ã™

        # ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        # (æ³¨: 'gemini-1.5-pro-latest' ã¯ APIã‚­ãƒ¼ã®ã¿ã§ã®èªè¨¼ (API Key) ã«å¯¾å¿œã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹)
        # (æ³¨: APIã‚­ãƒ¼èªè¨¼ã®å ´åˆã¯ 'gemini-pro' (1.0) ã‚’ä½¿ã†ã®ãŒç¢ºå®Ÿ)
        
        # å®‰å®šæ¿ã® 'gemini-pro' ã‚’ä½¿ç”¨
        model = genai.GenerativeModel('gemini-pro')
        
        # (ã‚ªãƒ—ã‚·ãƒ§ãƒ³: 1.5-flash ã‚’è©¦ã™å ´åˆ)
        # model = genai.GenerativeModel('gemini-1.5-flash-latest')

        # APIã‚­ãƒ¼ã‚’æ¸¡ã—ã¦åˆæœŸåŒ– (genai.configureãŒä½¿ãˆãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
        if not hasattr(genai, "configure"):
            model = genai.GenerativeModel('gemini-pro', api_key=api_key)


        # (æ³¨: 'gemini-1.5-pro-latest' ã¯ 2024/11 æ™‚ç‚¹ã§ APIã‚­ãƒ¼èªè¨¼ (API Key) ã«éå¯¾å¿œ)
        # model = genai.GenerativeModel('gemini-1.5-pro-latest')

        # ç–é€šç¢ºèª (ãƒ€ãƒŸãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ)
        # model.generate_content("test", generation_config={"max_output_tokens": 1})

        gemini_model = model
        print("âœ… Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸã€‚ (model: gemini-pro)")

    except Exception as e:
        print(f"  âŒ è­¦å‘Š: Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Geminiåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        gemini_model = None


def analyze_article_with_gemini(article_body):
    """
    è¨˜äº‹æœ¬æ–‡ã‚’å—ã‘å–ã‚Šã€Gemini API ã‚’ä½¿ã£ã¦
    sentiment, category, company_info, nissan_mention, nissan_sentiment ã‚’
    JSON å½¢å¼ã§è¿”ã™ã€‚
    """
    if not gemini_model:
        return {
            "sentiment": "N/A", "category": "N/A", "company_info": "N/A",
            "nissan_mention": "N/A", "nissan_sentiment": "N/A"
        }

    # æœ¬æ–‡ãŒé•·ã™ãã‚‹å ´åˆã€å…ˆé ­ã®10000æ–‡å­—ç¨‹åº¦ã«ä¸¸ã‚ã‚‹ (Geminiã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸Šé™å¯¾ç­–)
    max_length = 10000
    if len(article_body) > max_length:
        article_body = article_body[:max_length]

    # JSONãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ãŸã‚ã® GenerationConfig
    # (æ³¨: gemini-pro ã¯æ­£å¼ãªJSONãƒ¢ãƒ¼ãƒ‰ã«éå¯¾å¿œã€‚1.5ä»¥é™ãŒå¿…è¦)
    # (ã“ã“ã§ã¯ 1.0 pro ã‚’ä½¿ã†å‰æã§ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§JSONå‡ºåŠ›ã•ã›ã‚‹)
    # generation_config = {
    #     "response_mime_type": "application/json",
    # }

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿ç«‹ã¦ã‚‹
    # 1. å½¹å‰²
    # 2. è¨˜äº‹æœ¬æ–‡
    # 3. å„ã‚¿ã‚¹ã‚¯ (sentiment, category, company_info)
    # 4. JSONå‡ºåŠ›æŒ‡ç¤º
    
    full_prompt = f"""
{PROMPTS.get("role", "ã‚ãªãŸã¯æ¥­ç•Œã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚")}

ã€è¨˜äº‹æœ¬æ–‡ã€‘
{article_body}
ã€è¨˜äº‹æœ¬æ–‡ã“ã“ã¾ã§ã€‘

---
ã€ã‚¿ã‚¹ã‚¯ã€‘
è¨˜äº‹æœ¬æ–‡ã‚’åˆ†æã—ã€ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
çµæœã¯å¿…ãšæŒ‡å®šã•ã‚ŒãŸJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã€ã‚­ãƒ¼ã€Œsentimentã€ã€Œcategoryã€ã€Œcompany_infoã€ã€Œnissan_mentionã€ã€Œnissan_sentimentã€ã‚’æŒã¤å˜ä¸€ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

1. **sentimentã®åˆ¤å®š**:
{PROMPTS.get("sentiment", "ï¼ˆsentimentãƒ«ãƒ¼ãƒ«ï¼‰")}

2. **categoryã®åˆ¤å®š**:
{PROMPTS.get("category", "ï¼ˆcategoryãƒ«ãƒ¼ãƒ«ï¼‰")}

3. **company_infoã®åˆ¤å®š**:
{PROMPTS.get("company_info", "ï¼ˆcompany_infoãƒ«ãƒ¼ãƒ«ï¼‰")}

4. **nissan_mentionã®åˆ¤å®š**:
(æ³¨: company_infoãŒã€Œæ—¥ç”£ã€*ä»¥å¤–*ã®å ´åˆã®ã¿ã€æœ¬æ–‡ä¸­ã®ã€Œæ—¥ç”£ã€ã¸ã®è¨€åŠã‚’ç¢ºèªã›ã‚ˆ)
{PROMPTS.get("nissan_mention", "ï¼ˆnissan_mentionãƒ«ãƒ¼ãƒ«ï¼‰")}

5. **nissan_sentimentã®åˆ¤å®š**:
(æ³¨: nissan_mentionãŒã€Œ-ã€*ä»¥å¤–*ã®å ´åˆã®ã¿ã€ãã®è¨€åŠãŒæ—¥ç”£ã«ã¨ã£ã¦ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–/ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã‹åˆ¤å®šã›ã‚ˆ)
{PROMPTS.get("nissan_sentiment", "ï¼ˆnissan_sentimentãƒ«ãƒ¼ãƒ«ï¼‰")}

---
ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (JSON)ã€‘
{{
  "sentiment": "ï¼ˆ1ã®åˆ¤å®šçµæœï¼‰",
  "category": "ï¼ˆ2ã®åˆ¤å®šçµæœï¼‰",
  "company_info": "ï¼ˆ3ã®åˆ¤å®šçµæœï¼‰",
  "nissan_mention": "ï¼ˆ4ã®åˆ¤å®šçµæœï¼‰",
  "nissan_sentiment": "ï¼ˆ5ã®åˆ¤å®šçµæœï¼‰"
}}
"""

    try:
        # print(f"  [Debug] Gemini Prompt: {full_prompt[:200]}...") # ãƒ‡ãƒãƒƒã‚°ç”¨
        
        response = gemini_model.generate_content(full_prompt)
        
        # print(f"  [Debug] Gemini Response: {response.text}") # ãƒ‡ãƒãƒƒã‚°ç”¨

        # Gemini 1.0 Pro ã¯ JSON "ãƒ¢ãƒ¼ãƒ‰" ã«å¯¾å¿œã—ã¦ã„ãªã„ãŸã‚ã€
        # å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ JSON éƒ¨åˆ†ã‚’æŠ½å‡ºã™ã‚‹
        json_match = re.search(r"\{.*\}", response.text, re.DOTALL)
        
        if not json_match:
            print("  âŒ Geminiå¿œç­”ã‹ã‚‰JSONã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            print(f"  å¿œç­”: {response.text}")
            return {
                "sentiment": "N/A", "category": "N/A", "company_info": "N/A",
                "nissan_mention": "N/A", "nissan_sentiment": "N/A"
            }

        json_str = json_match.group(0)
        
        # JSONæ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
        result = json.loads(json_str)
        
        # å¿…è¦ãªã‚­ãƒ¼ãŒæƒã£ã¦ã„ã‚‹ã‹ç¢ºèª
        required_keys = ["sentiment", "category", "company_info", "nissan_mention", "nissan_sentiment"]
        if not all(key in result for key in required_keys):
             print(f"  âŒ Geminiå¿œç­”JSONã«å¿…è¦ãªã‚­ãƒ¼ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ {result.keys()}")
             # ä¸è¶³ã—ã¦ã„ã‚‹ã‚­ãƒ¼ã‚’ 'N/A' ã§è£œå®Œ
             for key in required_keys:
                 if key not in result:
                     result[key] = "N/A (ã‚­ãƒ¼æ¬ æ)"

        return result

    except json.JSONDecodeError as e:
        print(f"  âŒ Geminiå¿œç­”ã®JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        print(f"  å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ (JSONæŠ½å‡ºå¾Œ): {json_str}")
        return {
            "sentiment": "N/A", "category": "N/A", "company_info": "N/A",
            "nissan_mention": "N/A", "nissan_sentiment": "N/A"
        }
    except GoogleAPIError as e:
        print(f"  âŒ Gemini API ã‚¨ãƒ©ãƒ¼: {e}")
        # (ä¾‹: ã‚¯ã‚©ãƒ¼ã‚¿è¶…éã€èªè¨¼ã‚¨ãƒ©ãƒ¼ãªã©)
        return {
            "sentiment": "N/A", "category": "N/A", "company_info": "N/A",
            "nissan_mention": "N/A", "nissan_sentiment": "N/A"
        }
    except Exception as e:
        print(f"  âŒ Geminiåˆ†æä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return {
            "sentiment": "N/A", "category": "N/A", "company_info": "N/A",
            "nissan_mention": "N/A", "nissan_sentiment": "N/A"
        }


# --- (ä¿®æ­£ç®‡æ‰€) ---
# å•é¡Œ(3)å¯¾ç­–ï¼šã‚³ãƒ¡ãƒ³ãƒˆURLã®å½¢å¼å¤‰æ›´ã«ä¼´ã„ã€
# get_yahoo_news_comments é–¢æ•°ã®å¼•æ•°ã« article_url ã‚’è¿½åŠ 
def get_yahoo_news_comments(article_id, article_url):
    """
    è¨˜äº‹IDã¨è¨˜äº‹URLã‚’å—ã‘å–ã‚Šã€ã‚³ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸ã®1ã€œ3ãƒšãƒ¼ã‚¸ç›®ã¾ã§ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ã€‚
    """
    print(f"    - ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ (Såˆ—ï½ACåˆ—) ã‚’å–å¾—ä¸­...")
    comments_data = []
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        # --- (ä¿®æ­£ç®‡æ‰€) ---
        # å•é¡Œ(3)å¯¾ç­–ï¼šã‚³ãƒ¡ãƒ³ãƒˆURLã®å½¢å¼ã‚’å¤ã„ã‚‚ã®ã‹ã‚‰æ–°ã—ã„ã‚‚ã®ã¸å¤‰æ›´
        # æ—§: f"https://news.yahoo.co.jp/comments/{article_id}?page={page_num}"
        # æ–°: f"https://news.yahoo.co.jp/articles/{article_id}/comments?page={page_num}"
        # è¨˜äº‹URLå…¨ä½“ (article_url) ã‚’ä½¿ã†ã‚ˆã†ã«å¤‰æ›´ã—ã¾ã™ã€‚
        
        base_comments_url = f"{article_url}/comments"
        
        for page_num in range(1, 4): # 1ãƒšãƒ¼ã‚¸ã‹ã‚‰3ãƒšãƒ¼ã‚¸ã¾ã§
            if page_num == 1:
                comments_url = base_comments_url
            else:
                comments_url = f"{base_comments_url}?page={page_num}"

            # print(f"      - ã‚³ãƒ¡ãƒ³ãƒˆ ãƒšãƒ¼ã‚¸ {page_num} ( {comments_url} ) ã‚’å–å¾—ä¸­...") # ãƒ‡ãƒãƒƒã‚°ç”¨
            response = requests.get(comments_url, headers=headers)
            
            # ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„å ´åˆ (404ã‚¨ãƒ©ãƒ¼ãªã©)
            if response.status_code != 200:
                print(f"    âŒ ã‚³ãƒ¡ãƒ³ãƒˆ ãƒšãƒ¼ã‚¸ {page_num} ( {comments_url} ) ãŒå­˜åœ¨ã—ãªã„ã‹å–å¾—å¤±æ•—ã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
                break # å­˜åœ¨ã—ãªã„å ´åˆã¯ä»¥é™ã®ãƒšãƒ¼ã‚¸ã®ãƒã‚§ãƒƒã‚¯ã‚’ä¸­æ–­

            # --- (ä¿®æ­£ã“ã“ã¾ã§) ---

            soup = BeautifulSoup(response.text, "html.parser")

            # ã‚³ãƒ¡ãƒ³ãƒˆã®ã‚³ãƒ³ãƒ†ãƒŠã‚’æ¢ã™ (Yahoo!ã®HTMLæ§‹é€ ã«ä¾å­˜)
            # (æ³¨: ã“ã®ã‚»ãƒ¬ã‚¯ã‚¿ 'div.comment-list-item' ã‚‚å¤‰æ›´ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™)
            # (Yahoo!ã®ã‚¯ãƒ©ã‚¹åã¯é›£èª­åŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒå¤šã„)
            
            # é›£èª­åŒ–ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹åã«å¯¾å¿œã™ã‚‹ãŸã‚ã€éƒ¨åˆ†ä¸€è‡´ (class*='...') ã‚’ä½¿ç”¨
            comments = soup.select("div[class*='CommentItem__Container']") # æ–°ã—ã„å¯èƒ½æ€§ã®ã‚ã‚‹ã‚»ãƒ¬ã‚¯ã‚¿
            
            if not comments:
                 # ä»¥å‰ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚‚è©¦ã™ (ä¿é™º)
                comments = soup.select("div.comment-list-item") # å…ƒã®ã‚»ãƒ¬ã‚¯ã‚¿
            
            if not comments:
                # print(f"    - ã‚³ãƒ¡ãƒ³ãƒˆ ãƒšãƒ¼ã‚¸ {page_num} ã«ã‚³ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚") # ãƒ‡ãƒãƒƒã‚°ç”¨
                break # ãƒšãƒ¼ã‚¸ã¯ã‚ã‚‹ãŒã‚³ãƒ¡ãƒ³ãƒˆãŒç„¡ã„å ´åˆã‚‚ä¸­æ–­

            for comment in comments:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼å
                user_name_tag = comment.select_one("h3[class*='CommentItem__UserName']")
                user_name = user_name_tag.text.strip() if user_name_tag else "ãƒ¦ãƒ¼ã‚¶ãƒ¼åä¸æ˜"

                # ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡
                comment_text_tag = comment.select_one("p[class*='CommentItem__Text']")
                comment_text = comment_text_tag.text.strip() if comment_text_tag else "ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ãªã—"

                comments_data.append(f"ã€{user_name}ã€‘{comment_text}")

                if len(comments_data) >= 10: # 10ä»¶å–å¾—ã—ãŸã‚‰çµ‚äº†
                    break
            
            if len(comments_data) >= 10:
                break
            
            time.sleep(1) # 1ç§’å¾…æ©Ÿ

        # 10ä»¶ã«æº€ãŸãªã„å ´åˆã¯ã€Œ-ã€ã§åŸ‹ã‚ã‚‹
        if not comments_data:
            print(f"    - ã‚³ãƒ¡ãƒ³ãƒˆãŒ1ä»¶ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆã¾ãŸã¯ã‚³ãƒ¡ãƒ³ãƒˆæ¬„é–‰é–ï¼‰ã€‚")
            return ["å–å¾—ä¸å¯"] * 10

        if len(comments_data) < 10:
            comments_data.extend(["-"] * (10 - len(comments_data)))

        print(f"    âœ… ã‚³ãƒ¡ãƒ³ãƒˆ {len(comments_data)} ä»¶ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        return comments_data[:10]

    except Exception as e:
        print(f"    âŒ ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return ["å–å¾—ä¸å¯"] * 10


def update_source_sheet(ws, new_articles, existing_urls):
    """
    SOURCE ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‚’æ›´æ–°ã™ã‚‹ã€‚
    1. æ–°ã—ã„è¨˜äº‹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    2. æ–°ã—ã„è¨˜äº‹ã‚’ã‚·ãƒ¼ãƒˆã«è¿½åŠ  (A-Eåˆ—)
    3. analysis_flag ãŒ "TRUE" ã‹ã¤ æœ¬æ–‡ãŒç©ºã®è¨˜äº‹ (F-ACåˆ—) ã‚’æ›´æ–°
    """
    
    # --- 1. æ–°ã—ã„è¨˜äº‹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ---
    articles_to_add = []
    for article in new_articles:
        if article["url"] not in existing_urls:
            
            # æŠ•ç¨¿æ—¥æ™‚ã‚’ãƒ‘ãƒ¼ã‚¹
            post_time = parse_relative_time(article["post_time_str"])
            if post_time:
                # Google Sheets ãŒèªè­˜ã§ãã‚‹å½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                post_time_formatted = post_time.strftime("%Y/%m/%d %H:%M:%S")
            else:
                post_time_formatted = article["post_time_str"] # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ãã®ã¾ã¾

            # Aåˆ—: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰, Båˆ—: URL, Cåˆ—: æŠ•ç¨¿æ—¥æ™‚, Dåˆ—: ç™ºè¡Œå…ƒ, Eåˆ—: ã‚¿ã‚¤ãƒˆãƒ«
            row_data = [
                article["keyword"],
                article["url"],
                post_time_formatted,
                article["source"],
                article["title"],
                "TRUE" # Fåˆ—: analysis_flag (æ–°è¦è¿½åŠ æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆTRUE)
            ]
            articles_to_add.append(row_data)
            
            # ãƒ¡ãƒ¢ãƒªä¸Šã® existing_urls ã«ã‚‚è¿½åŠ  (é‡è¤‡è¿½åŠ é˜²æ­¢)
            existing_urls.add(article["url"])

    # --- 2. æ–°ã—ã„è¨˜äº‹ã‚’ã‚·ãƒ¼ãƒˆã«è¿½åŠ  ---
    if articles_to_add:
        try:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æœ€çµ‚è¡Œã®æ¬¡ã«è¿½åŠ 
            ws.append_rows(articles_to_add, value_input_option="USER_ENTERED")
            print(f"  âœ… {len(articles_to_add)} ä»¶ã®æ–°ã—ã„è¨˜äº‹ã‚’ SOURCEã‚·ãƒ¼ãƒˆ ã«è¿½åŠ ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"  âŒ æ–°è¦è¨˜äº‹ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            # ã“ã“ã§å¤±æ•—ã—ã¦ã‚‚ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆæœ¬æ–‡å–å¾—ï¼‰ã¯è©¦ã¿ã‚‹
    else:
        print("  SOURCEã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã™ã¹ãæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")


    # --- 3. æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆç­‰ãŒæœªå–å¾—ã®è¨˜äº‹ã‚’æ›´æ–° ---
    try:
        # Aåˆ—ã‹ã‚‰ACåˆ— (1~29åˆ—) ã¾ã§ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        print("  ... æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæœªå–å¾—ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­ ...")
        all_data = ws.get_all_values()
        if len(all_data) <= 1:
            print("  - ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return # ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿

        headers = all_data[0]
        data_rows = all_data[1:]
        
        # åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç‰¹å®š (0å§‹ã¾ã‚Š)
        try:
            url_col = headers.index("URL") # Båˆ—
            flag_col = headers.index("analysis_flag") # Fåˆ—
            body_p1_col = headers.index("body_p1") # Gåˆ—
            comment_count_col = headers.index("comment_count") # Qåˆ—
            full_time_col = headers.index("full_post_time") # Råˆ—
            comment_1_col = headers.index("comment_1") # Såˆ—
        except ValueError as e:
            print(f"  âŒ å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}ã€‚æœ¬æ–‡å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return

        # æ›´æ–°ãƒ‡ãƒ¼ã‚¿ã‚’æºœã‚è¾¼ã‚€ãƒªã‚¹ãƒˆ (gspread ãƒãƒƒãƒæ›´æ–°ç”¨)
        batch_update_data = []

        # 2è¡Œç›®ã‹ã‚‰ (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ 0 = 2è¡Œç›®)
        for i, row in enumerate(data_rows):
            row_index = i + 2 # å®Ÿéš›ã®ã‚·ãƒ¼ãƒˆä¸Šã®è¡Œç•ªå·
            
            # è¡ŒãŒçŸ­ã™ãã‚‹å ´åˆ (é€”ä¸­ã®ç©ºè¡Œãªã©) ã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(row) <= max(flag_col, body_p1_col, url_col):
                continue
            
            analysis_flag = row[flag_col]
            body_p1 = row[body_p1_col]
            
            # (analysis_flagãŒTRUE ã‹ã¤ body_p1ãŒç©º ã¾ãŸã¯ 'ï¼ˆæœ¬æ–‡å–å¾—å¤±æ•—ï¼‰') ã®å ´åˆã«å®Ÿè¡Œ
            if (analysis_flag.upper() == "TRUE" or analysis_flag == "1") and \
               (not body_p1 or body_p1 == "ï¼ˆæœ¬æ–‡å–å¾—å¤±æ•—ï¼‰"):
                
                print(f"  - è¡Œ {row_index} (è¨˜äº‹: {row[4][:30]}...): æœ¬æ–‡(P1-P10)/ã‚³ãƒ¡ãƒ³ãƒˆæ•°/æ—¥æ™‚è£œå®Œ/ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ ã‚’å–å¾—ä¸­... (å®Œå…¨å–å¾—)")
                
                article_url = row[url_col]
                # è¨˜äº‹IDã‚’URLã‹ã‚‰æŠ½å‡º
                article_id_match = re.search(r"/articles/([a-f0-9]+)", article_url)
                if not article_id_match:
                    print(f"    - URLã‹ã‚‰è¨˜äº‹IDãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {article_url}")
                    continue
                
                article_id = article_id_match.group(1)

                # è©³ç´°ã‚’å–å¾—
                article_body_parts, comment_count, full_post_time = get_article_details(article_url)
                
                # --- (ä¿®æ­£ç®‡æ‰€) ---
                # å•é¡Œ(3)å¯¾ç­–ï¼šget_yahoo_news_comments ã« article_url ã‚’æ¸¡ã™
                comments_data = get_yahoo_news_comments(article_id, article_url)
                # --- (ä¿®æ­£ã“ã“ã¾ã§) ---
                
                # æ›´æ–°ç”¨ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                update_row_data = []
                update_row_data.extend(article_body_parts) # G-Påˆ— (10åˆ—)
                update_row_data.append(comment_count) # Qåˆ—
                
                # Råˆ— (full_post_time)
                if full_post_time:
                    # 'YYYY/MM/DD HH:MM:SS' å½¢å¼ã«
                    jst = full_post_time.astimezone(timedelta(hours=9))
                    update_row_data.append(jst.strftime("%Y/%m/%d %H:%M:%S"))
                else:
                    update_row_data.append("-") # å–å¾—å¤±æ•—æ™‚ã¯ãƒã‚¤ãƒ•ãƒ³

                update_row_data.extend(comments_data) # S-ACåˆ— (10åˆ—)
                
                # æ›´æ–°ç¯„å›² (Gåˆ— ã‹ã‚‰ ACåˆ— ã¾ã§)
                start_col_letter = gspread.utils.rowcol_to_a1(row_index, body_p1_col + 1)[0]
                end_col_letter = gspread.utils.rowcol_to_a1(row_index, comment_1_col + 9)
                end_col_letter = ''.join([c for c in end_col_letter if not c.isdigit()]) # 'AC' ãªã©

                range_to_update = f"{start_col_letter}{row_index}:{end_col_letter}{row_index}"
                
                batch_update_data.append({
                    'range': range_to_update,
                    'values': [update_row_data]
                })

                # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ 3ç§’å¾…æ©Ÿ
                time.sleep(3)
        
        # --- 4. æºœã‚è¾¼ã‚“ã æ›´æ–°ã‚’ä¸€æ‹¬å®Ÿè¡Œ ---
        if batch_update_data:
            print(f"  ... {len(batch_update_data)} ä»¶ã®æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¸€æ‹¬æ›¸ãè¾¼ã¿ä¸­ ...")
            ws.batch_update(batch_update_data, value_input_option="USER_ENTERED")
            print("  âœ… æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬æ›¸ãè¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        print(f"  âŒ æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ãƒ»æ›¸ãè¾¼ã¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()


def sort_and_format_sheet(gc):
    """
    SOURCE ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã® Cåˆ— (æŠ•ç¨¿æ—¥æ™‚) ã®æ›¸å¼ã‚’æ•´ãˆã€
    ã‚·ãƒ¼ãƒˆå…¨ä½“ã‚’ Cåˆ— ã®é™é † (æ–°ã—ã„é †) ã§ã‚½ãƒ¼ãƒˆã™ã‚‹ã€‚
    """
    print("\n===== ğŸ“‘ ã‚¹ãƒ†ãƒƒãƒ—â‘¢ è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ã‚½ãƒ¼ãƒˆã¨æ•´å½¢ =====")
    ws = get_worksheet(gc, "SOURCE")
    if not ws:
        return

    try:
        # Cåˆ— (æŠ•ç¨¿æ—¥æ™‚) ã®æ›¸å¼ã‚’ 'yyyy/mm/dd hh:mm:ss' ã«è¨­å®š
        # (æ³¨: gspread v6 ã§ã¯ set_basic_filter ãŒãªã„å ´åˆãŒã‚ã‚‹)
        
        # A1è¡¨è¨˜ã§è¡Œæ•°ãƒ»åˆ—æ•°ã‚’å–å¾—
        end_cell = gspread.utils.rowcol_to_a1(ws.row_count, ws.col_count)
        
        # Cåˆ—å…¨ä½“ã®æ›¸å¼è¨­å®šãƒªã‚¯ã‚¨ã‚¹ãƒˆ (C2ã‹ã‚‰Cåˆ—æœ€å¾Œã¾ã§)
        format_request = {
            "repeatCell": {
                "range": {
                    "sheetId": ws.id,
                    "startRowIndex": 1,  # 2è¡Œç›®ã‹ã‚‰ (0-indexed)
                    "endRowIndex": ws.row_count,
                    "startColumnIndex": 2, # Cåˆ— (0-indexed)
                    "endColumnIndex": 3
                },
                "cell": {
                    "userEnteredFormat": {
                        "numberFormat": {
                            "type": "DATE_TIME",
                            "pattern": "yyyy/mm/dd hh:mm:ss"
                        }
                    }
                },
                "fields": "userEnteredFormat.numberFormat"
            }
        }

        # ã‚½ãƒ¼ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ (Cåˆ—=åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹2 ã§é™é †ã‚½ãƒ¼ãƒˆ)
        sort_request = {
            "sortRange": {
                "range": {
                    "sheetId": ws.id,
                    "startRowIndex": 1, # 2è¡Œç›®ã‹ã‚‰ (ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã)
                    "endRowIndex": ws.row_count,
                    "startColumnIndex": 0, # Aåˆ—ã‹ã‚‰
                    "endColumnIndex": ws.col_count
                },
                "sortSpecs": [
                    {
                        "dimensionIndex": 2, # Cåˆ— (0-indexed)
                        "sortOrder": "DESCENDING"
                    }
                ]
            }
        }
        
        # (gspread v5.x ä»¥å‰ã®æ–¹æ³•: Cåˆ—ã®æ›œæ—¥ (æœˆ) ãªã©ã‚’é™¤å»)
        # (ã“ã‚Œã¯ API v4 ã§ã¯ä¸è¦ã€‚æ›¸å¼è¨­å®šã§å¯¾å¿œ)
        print(" ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§Cåˆ—ã®æ›¸å¼è¨­å®šã¨ã‚½ãƒ¼ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        
        # ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã§æ›¸å¼è¨­å®šã¨ã‚½ãƒ¼ãƒˆã‚’åŒæ™‚ã«å®Ÿè¡Œ
        ws.spreadsheet.batch_update({
            "requests": [format_request, sort_request]
        })
        
        print(f" âœ… Cåˆ—(2è¡Œç›®ã€œ{ws.row_count}è¡Œ) ã®è¡¨ç¤ºå½¢å¼ã‚’ 'yyyy/mm/dd hh:mm:ss' ã«è¨­å®šã—ã¾ã—ãŸã€‚")
        print(" âœ… SOURCEã‚·ãƒ¼ãƒˆã‚’æŠ•ç¨¿æ—¥æ™‚ã®æ–°ã—ã„é †ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§ä¸¦ã³æ›¿ãˆã¾ã—ãŸã€‚")

    except Exception as e:
        print(f"  âŒ ã‚½ãƒ¼ãƒˆãƒ»æ›¸å¼è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        print("  (æ³¨: Google Sheets API v4 ãŒå¿…è¦ã§ã™)")


# --- (ä¿®æ­£ç®‡æ‰€) ---
# å•é¡Œ(1)å¯¾ç­–ï¼šgspread API ã® 429 ã‚¨ãƒ©ãƒ¼ (Quota Exceeded) ã‚’å›é¿ã™ã‚‹ãŸã‚ã€
# 1ä»¶ãšã¤ ws.update ã™ã‚‹ã®ã§ã¯ãªãã€ws.batch_update ã§ä¸€æ‹¬æ›¸ãè¾¼ã¿ã™ã‚‹
def analyze_with_gemini_and_update_sheet(gc):
    """
    ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã€Œåˆ†æãƒ•ãƒ©ã‚°ã€ãŒç«‹ã£ã¦ã„ã‚‹è¨˜äº‹ï¼ˆæœ€å¤§30ä»¶ï¼‰ã‚’Geminiã§åˆ†æã—ã€
    çµæœã‚’P-Råˆ— (sentiment, category, company_info) ã¨
    AD-AEåˆ— (nissan_mention, nissan_sentiment) ã«ä¸€æ‹¬ã§æ›¸ãè¾¼ã‚€ã€‚
    """
    try:
        if not gemini_model:
            print("\n===== ğŸ§  ã‚¹ãƒ†ãƒƒãƒ—â‘£ (ã‚¹ã‚­ãƒƒãƒ—) =====")
            print("  Geminiãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return

        print("\n===== ğŸ§  ã‚¹ãƒ†ãƒƒãƒ—â‘£ Geminiåˆ†æã®å®Ÿè¡Œãƒ»å³æ™‚åæ˜  (P-R, AD-AEåˆ—) [æœ€å¤§30ä»¶] =====")
        ws = get_worksheet(gc, "SOURCE")
        if not ws:
            return

        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’å–å¾—ã—ã¦ã€åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‹•çš„ã«è¦‹ã¤ã‘ã‚‹
        headers = ws.row_values(1)
        try:
            # å¿…è¦ãªåˆ—ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0å§‹ã¾ã‚Šï¼‰ã‚’å–å¾—
            title_col_idx = headers.index("title") + 1 # Eåˆ—
            analysis_flag_col_idx = headers.index("analysis_flag") + 1 # Fåˆ—
            body_col_idx = headers.index("body_p1") + 1 # Gåˆ—
            sentiment_col_idx = headers.index("sentiment") + 1 # Påˆ—
            category_col_idx = headers.index("category") + 1 # Qåˆ—
            company_info_col_idx = headers.index("company_info") + 1 # Råˆ—
            nissan_mention_col_idx = headers.index("nissan_mention") + 1 # ADåˆ—
            nissan_sentiment_col_idx = headers.index("nissan_sentiment") + 1 # AEåˆ—

        except ValueError as e:
            print(f"  âŒ å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}ã€‚åˆ†æã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
            return

        # Aåˆ—ã‹ã‚‰AEåˆ—ã¾ã§ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«å–å¾—
        print("  ... åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­ ...")
        all_data = ws.get_all_values()
        if len(all_data) <= 1:
            print("  åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return

        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’é™¤ã„ãŸãƒ‡ãƒ¼ã‚¿æœ¬ä½“
        data_rows = all_data[1:]
        
        # --- (ä¿®æ­£ç®‡æ‰€) ---
        # å•é¡Œ(1)å¯¾ç­–ï¼šæ›¸ãè¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ™‚çš„ã«æºœã‚è¾¼ã‚€ãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–
        batch_updates = []
        # --- (ä¿®æ­£ã“ã“ã¾ã§) ---

        count = 0
        max_analyze = 30 # æœ€å¤§åˆ†æä»¶æ•°

        # 2è¡Œç›®ã‹ã‚‰ (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹0 = 2è¡Œç›®)
        for i, row in enumerate(data_rows):
            row_index = i + 2 # å®Ÿéš›ã®ã‚·ãƒ¼ãƒˆä¸Šã®è¡Œç•ªå·
            
            # è¡Œãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(row) <= max(analysis_flag_col_idx-1, sentiment_col_idx-1, body_col_idx-1):
                continue

            try:
                # åˆ†æãƒ•ãƒ©ã‚° (analysis_flag_col_idx-1) ã¨ åˆ†æçµæœ (sentiment_col_idx-1) ã‚’ãƒã‚§ãƒƒã‚¯
                analysis_flag = row[analysis_flag_col_idx - 1]
                sentiment = row[sentiment_col_idx - 1]
                
                # åˆ†æãƒ•ãƒ©ã‚°ãŒ 'TRUE' (ã¾ãŸã¯ '1') ã§ã€ã‹ã¤ sentiment ãŒç©º (ã¾ãŸã¯ 'N/A') ã®å ´åˆã®ã¿å®Ÿè¡Œ
                if (analysis_flag.upper() == "TRUE" or analysis_flag == "1") and (not sentiment or sentiment == "N/A"):
                    
                    if count >= max_analyze:
                        print(f"  åˆ†æä»¶æ•°ãŒ{max_analyze}ä»¶ã«é”ã—ãŸãŸã‚ã€æ®‹ã‚Šã¯æ¬¡å›ã«å›ã—ã¾ã™ã€‚")
                        break
                    
                    count += 1
                    title = row[title_col_idx - 1][:30] # ã‚¿ã‚¤ãƒˆãƒ«åˆ—
                    print(f"  - è¡Œ {row_index} (è¨˜äº‹: {title}...): Geminiåˆ†æã‚’å®Ÿè¡Œä¸­... ({count}/{max_analyze}ä»¶ç›®)")

                    # æœ¬æ–‡ (Gåˆ—ã‹ã‚‰Påˆ—ã®ç›´å‰ã¾ã§)
                    body_p1_to_p10 = row[body_col_idx - 1 : body_col_idx + 9]
                    article_body = " ".join([text for text in body_p1_to_p10 if text and text != "-"])
                    
                    if len(article_body.strip()) < 50: # æœ¬æ–‡ãŒçŸ­ã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        print(f"    ...æœ¬æ–‡ãŒçŸ­ã™ãã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ— (æœ¬æ–‡: {article_body[:50]}...)")
                        # ã‚¹ã‚­ãƒƒãƒ—ã—ãŸå ´åˆã§ã‚‚ã€ãƒ•ãƒ©ã‚°ã‚’ 'FALSE' ã«ã—ã¦æ¬¡å›ä»¥é™ã®ç„¡é§„ãªãƒã‚§ãƒƒã‚¯ã‚’é˜²ã
                        analysis_result = {
                            "sentiment": "N/A (æœ¬æ–‡çŸ­)", "category": "N/A", "company_info": "N/A",
                            "nissan_mention": "-", "nissan_sentiment": "-"
                        }
                    else:
                        # Gemini APIã«åˆ†æã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                        analysis_result = analyze_article_with_gemini(article_body)
                    
                    # åˆ†æçµæœã‚’å„å¤‰æ•°ã«æ ¼ç´
                    sentiment = analysis_result.get("sentiment", "N/A")
                    category = analysis_result.get("category", "N/A")
                    company_info = analysis_result.get("company_info", "N/A")
                    nissan_mention = analysis_result.get("nissan_mention", "N/A")
                    nissan_sentiment = analysis_result.get("nissan_sentiment", "N/A")

                    # --- (ä¿®æ­£ç®‡æ‰€) ---
                    # å•é¡Œ(1)å¯¾ç­–ï¼šws.update() ã‚’å‘¼ã³å‡ºã™ä»£ã‚ã‚Šã«ã€batch_updatesãƒªã‚¹ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã™ã‚‹
                    
                    # ãƒ¡ã‚¤ãƒ³ã®åˆ†æçµæœ (Påˆ—ã€œRåˆ—)
                    batch_updates.append({
                        'range': f"{gspread.utils.rowcol_to_a1(row_index, sentiment_col_idx)}:{gspread.utils.rowcol_to_a1(row_index, company_info_col_idx)}",
                        'values': [[sentiment, category, company_info]]
                    })
                    
                    # æ—¥ç”£é–¢é€£ã®åˆ†æçµæœ (ADåˆ—ã€œAEåˆ—)
                    batch_updates.append({
                        'range': f"{gspread.utils.rowcol_to_a1(row_index, nissan_mention_col_idx)}:{gspread.utils.rowcol_to_a1(row_index, nissan_sentiment_col_idx)}",
                        'values': [[nissan_mention, nissan_sentiment]]
                    })
                    
                    # (æ³¨: åˆ†æãƒ•ãƒ©ã‚° Fåˆ— ã‚’ 'FALSE' ã«ã™ã‚‹å‡¦ç†ã¯ã“ã“ã«ã¯ç„¡ã„)
                    # (ã‚‚ã— 'FALSE' ã«ã—ãŸã„ãªã‚‰ã€ã“ã“ã«ã‚‚ã†1ã¤ append ã‚’è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚‹)
                    
                    # --- (ä¿®æ­£ã“ã“ã¾ã§) ---

                    time.sleep(1) # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã®é–“ã«çŸ­ã„å¾…æ©Ÿ (Gemini APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–)

            except Exception as e:
                print(f"  âŒ è¡Œ {row_index} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                traceback.print_exc()

        # --- (ä¿®æ­£ç®‡æ‰€) ---
        # å•é¡Œ(1)å¯¾ç­–ï¼šãƒ«ãƒ¼ãƒ—ãŒå…¨ã¦çµ‚ã‚ã£ãŸå¾Œã€æºœã‚è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ã§æ›¸ãè¾¼ã‚€
        if batch_updates:
            print(f"  ... {len(batch_updates) // 2} ä»¶ã®åˆ†æçµæœã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¸€æ‹¬æ›¸ãè¾¼ã¿ä¸­ ...")
            try:
                ws.batch_update(batch_updates, value_input_option="USER_ENTERED")
                print("  âœ… åˆ†æçµæœã®ä¸€æ‹¬æ›¸ãè¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            except Exception as e:
                print(f"  âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®ä¸€æ‹¬æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                # (ã“ã“ã§ 429 Quota Exceeded ãŒå‡ºã‚‹å ´åˆã¯ã€batch_updates ã®é‡ãŒå¤šã™ãã‚‹å¯èƒ½æ€§)
                traceback.print_exc()
        elif count == 0:
            print("  åˆ†æå¯¾è±¡ï¼ˆåˆ†æãƒ•ãƒ©ã‚°ãŒTRUEã§æœªåˆ†æï¼‰ã®è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        # --- (ä¿®æ­£ã“ã“ã¾ã§) ---

    except Exception as e:
        print(f"  âŒ Geminiåˆ†æã‚¹ãƒ†ãƒƒãƒ—å…¨ä½“ã§ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()


def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    print("--- çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹ ---")
    start_time = time.time()
    
    # --- ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ---
    gc = setup_gspread()
    if not gc:
        print("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆèªè¨¼ã«å¤±æ•—ã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    ws = get_worksheet(gc, "SOURCE")
    if not ws:
        print("SOURCE ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã®å–å¾—ã«å¤±æ•—ã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return
        
    if not load_prompts():
        print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿ã«å¤±æ•—ã€‚Geminiåˆ†æã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã€‚")
        # (å‡¦ç†ã¯ç¶šè¡Œ)

    initialize_gemini() # Gemini APIã®åˆæœŸåŒ–

    # --- ã‚¹ãƒ†ãƒƒãƒ—â‘  ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆå–å¾— & ã‚¹ãƒ†ãƒƒãƒ—â‘¡ æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— ---
    existing_urls = load_existing_urls(ws)
    print(f"  (ç¾åœ¨ {len(existing_urls)} ä»¶ã®è¨˜äº‹URLã‚’ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿)")
    
    all_new_articles = []
    for keyword in SEARCH_KEYWORDS:
        print(f"\n===== ğŸ”‘ ã‚¹ãƒ†ãƒƒãƒ—â‘  ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆå–å¾—: {keyword} =====")
        new_articles = get_yahoo_news_search_results(keyword)
        
        # ã‚¹ãƒ†ãƒƒãƒ—â‘¡ (æ›´æ–°å‡¦ç†)
        # (æ³¨: æœ¬æ¥ã¯ã‚¹ãƒ†ãƒƒãƒ—â‘ ã‚’å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†ã‚„ã£ã¦ã‹ã‚‰â‘¡ã‚’ã‚„ã‚‹ã¹ãã ãŒã€
        #  å…ƒã®ã‚³ãƒ¼ãƒ‰ ã§ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã«â‘¡ã‚’å®Ÿè¡Œã—ã¦ã„ãŸãŸã‚ã€ãã‚Œã‚’è¸è¥²)
        
        # (å…ƒã®ã‚³ãƒ¼ãƒ‰ ã®ãƒ­ã‚¸ãƒƒã‚¯ã«å¾“ã„ã€
        #  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã«æ–°è¦è¿½åŠ ã¨ã€å…¨ãƒ‡ãƒ¼ã‚¿ã®æœ¬æ–‡å–å¾—ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ)
        print(f"\n===== ğŸ“ ã‚¹ãƒ†ãƒƒãƒ—â‘¡ æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆæ›´æ–° (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword} è¿½åŠ å¾Œ) =====")
        update_source_sheet(ws, new_articles, existing_urls)
        
        # (æ³¨: ã“ã®è¨­è¨ˆã ã¨ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œãƒˆãƒ¨ã‚¿ã€ã®å®Ÿè¡Œæ™‚ã«ã€
        #  ã€Œæ—¥ç”£ã€ã®å¤ã„æœªå–å¾—ãƒ‡ãƒ¼ã‚¿ã‚‚å–å¾—ã—ã«ã„ãã€‚
        #  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œæ—¥ç”£ã€ã®å®Ÿè¡Œæ™‚ã«ã‚‚ã€å†åº¦ã€Œãƒˆãƒ¨ã‚¿ã€ã®æœªå–å¾—ãƒ‡ãƒ¼ã‚¿ã‚‚ãƒã‚§ãƒƒã‚¯ã—ã«ã„ãã€‚
        #  éåŠ¹ç‡ã ãŒã€å…ƒã®è¨­è¨ˆ ã‚’ç¶­æŒã™ã‚‹)


    # --- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ã‚½ãƒ¼ãƒˆ & æ›¸å¼è¨­å®š ---
    # (å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡¦ç†ãŒçµ‚ã‚ã£ãŸå¾Œã«1å›ã ã‘å®Ÿè¡Œ)
    sort_and_format_sheet(gc)

    # --- ã‚¹ãƒ†ãƒƒãƒ—â‘£ Gemini åˆ†æ ---
    # (å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡¦ç†ãŒçµ‚ã‚ã£ãŸå¾Œã«1å›ã ã‘å®Ÿè¡Œ)
    analyze_with_gemini_and_update_sheet(gc)

    end_time = time.time()
    print(f"\n--- çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆçµ‚äº† (æ‰€è¦æ™‚é–“: {end_time - start_time:.2f}ç§’) ---")


if __name__ == "__main__":
    main()
